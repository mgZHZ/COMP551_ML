{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5LYr_bzYM4O"
      },
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "from google.colab import files\n",
        "\n",
        "# Load file for google colab, comment out for local test\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# scale function to rescale the continuous feature into value between 0 and 1\n",
        "def maximum_absolute_scaling(df):\n",
        "  # copy the dataframe\n",
        "  df_scaled = df.copy()\n",
        "  # apply maximum absolute scaling\n",
        "  df_scaled.iloc[:, 2:18] = df_scaled.iloc[:, 2:18] / df_scaled.iloc[:, 2:18].abs().max()\n",
        "  return df_scaled\n",
        "\n",
        "def load_and_clean_data():\n",
        "  # Load file for google colab, comment out for local test\n",
        "  #uploaded = files.upload() \n",
        "  # loading dataset \n",
        "  data = arff.loadarff('messidor_features.arff')\n",
        "  dataset2 = pd.DataFrame(data[0])\n",
        "  dataset2.columns = ['Quality', 'Pre-screening', 'MA-0.5', 'MA-0.6', 'MA-0.7', 'MA-0.8', 'MA-0.9', 'MA-1.0', 'Exudates-1', 'Exudates-2', 'Exudates-3', 'Exudates-4', 'Exudates-5', 'Exudates-6','Exudates-7','Exudates-8','dist-macula-optic', 'diameter-optic', 'AM/FM-classi', 'Class']\n",
        "  # Clean data - remove all rows with missing values\n",
        "  dataset2=dataset2.replace(\"?\",np.nan).dropna()\n",
        "  dataset2['Class'] = dataset2['Class'].str.decode('utf-8') \n",
        "  dataset2 = dataset2.apply(pd.to_numeric)\n",
        "  #x, y = dataset2.iloc[: , :-1].to_numpy(), dataset2.iloc[: , -1].to_numpy()  \n",
        "  #print(dataset2)\n",
        "  return dataset2\n",
        "\n",
        "def scaled_data(dataset2):\n",
        "  df_scaled = maximum_absolute_scaling(dataset2)\n",
        "  x, y = df_scaled.iloc[: , :-1].to_numpy(), df_scaled.iloc[: , -1].to_numpy()  \n",
        "  (N,D), C = x.shape, np.max(y)+1\n",
        "  print(f'instances (N) \\t {N} \\nfeatures (D) \\t {D} \\nclasses (C) \\t {C}')    \n",
        "  return x, y, N\n",
        "\n",
        "def unscaled_data(dataset2):\n",
        "  x, y = dataset2.iloc[:, :-1].to_numpy(), dataset2.iloc[:, -1].to_numpy()\n",
        "  (N, D), C = x.shape, np.max(y) + 1\n",
        "  print(f'instances (N) \\t {N} \\nfeatures (D) \\t {D} \\nclasses (C) \\t {C}')\n",
        "  return x,y,N\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpQyKGD3Pl4Z"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k9s_uh2JKuL"
      },
      "outputs": [],
      "source": [
        "#define the metric we will use to measure similarity \n",
        "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2,axis = -1))\n",
        "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, K=1, dist_fn= euclidean):\n",
        "        self.dist_fn = dist_fn                                                    #we need to use self because the parameters would be stored in variables \n",
        "                                                                                  #on the stack and would be discarded when the init method goes out of scope\n",
        "        self.K = K\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        ''' Store the training data using this method as it is a lazy learner'''\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.C = np.max(y) + 1\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x_test):\n",
        "        ''' Makes a prediction using the stored training data and the test data given as argument'''\n",
        "\n",
        "        num_test = x_test.shape[0]\n",
        "        #calculate distance between the training & test samples and returns an array of shape [num_test, num_train]    \n",
        "        distances=distances = self.dist_fn(self.x[None, :, :], x_test[:, None, :])\n",
        "        \n",
        "        #ith-row of knns stores the indices of k closest training samples to the ith-test sample \n",
        "        knns = np.zeros((num_test, self.K), dtype=int)\n",
        "        \n",
        "        #ith-row of y_prob has the probability distribution over C classes\n",
        "        y_prob = np.zeros((num_test, self.C))\n",
        "        for i in range(num_test):\n",
        "            knns[i,:] = np.argsort(distances[i])[:self.K]  \n",
        "            #counts the number of instances of each class in the K-closest training samples\n",
        "            y_prob[i,:] = np.bincount(self.y[knns[i,:]], minlength=self.C) \n",
        "        y_prob /= self.K                                                          \n",
        "        return y_prob, knns\n",
        "\n",
        "def model_knn_for_specific_k(x, y, N, k):\n",
        "  model = KNN(K=k)\n",
        "  accuracy_list=[]\n",
        "  for i in range(100):\n",
        "    inds = np.random.permutation(N)    \n",
        "    #split the dataset into train and test(80% training & 20% testing)\n",
        "    x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "    x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "    y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "    y_pred = np.argmax(y_prob,axis=-1)                                                \n",
        "    accuracy_list.append(np.sum(y_pred == y_test)/y_test.shape[0])\n",
        "  print('knns shape:', knns.shape)\n",
        "  print('y_prob shape:', y_prob.shape)\n",
        "\n",
        "  accuracy=sum(accuracy_list) /len(accuracy_list)\n",
        "\n",
        "  print(f'accuracy is {accuracy*100:.1f}.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN for different distance function - plot"
      ],
      "metadata": {
        "id": "y2iCK8NtmYo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction_cost(N, k, x, y):\n",
        "    model1 = KNN(K=k,dist_fn= euclidean)\n",
        "    model2 = KNN(K=k,dist_fn= manhattan)\n",
        "    #square_error = []\n",
        "    accuracy_list1=[]\n",
        "    accuracy_list2=[]\n",
        "    for i in range(50):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "        x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "        y_prob, knns = model1.fit(x_train, y_train).predict(x_test)\n",
        "        y_prob1, knns1 = model2.fit(x_train, y_train).predict(x_test)\n",
        "        # To get hard predictions by choosing the class with the maximum probability\n",
        "        y_pred = np.argmax(y_prob, axis=-1)\n",
        "        y_pred1 = np.argmax(y_prob1, axis=-1)\n",
        "        accuracy_list1.append(np.sum(y_pred == y_test) / y_test.shape[0])\n",
        "        accuracy_list2.append(np.sum(y_pred1 == y_test) / y_test.shape[0])\n",
        "    accuracy_euclidean=sum(accuracy_list1) /len(accuracy_list1)\n",
        "    accuracy_manhattan=sum(accuracy_list2) /len(accuracy_list2)\n",
        "    # print(f'accuracy is {accuracy * 100:.1f}.')\n",
        "    # print(f'mean squared error is {mean_square_error}.')\n",
        "    return accuracy_euclidean, accuracy_manhattan\n",
        "\n",
        "def compare_diff_distance_func(x,y,N):\n",
        "    k_list=[]\n",
        "    accuracy_euclidean_list=[]\n",
        "    accuracy_manhattan_list=[]\n",
        "    for k in range(1,21):\n",
        "        accuracy_euclidean, accuracy_manhattan=make_prediction_cost(N, k, x, y)\n",
        "        accuracy_euclidean_list.append(accuracy_euclidean)\n",
        "        accuracy_manhattan_list.append(accuracy_manhattan)\n",
        "        k_list.append(k)\n",
        "    plt.plot(k_list, accuracy_euclidean_list, label=\"euclidean\")\n",
        "    plt.plot(k_list, accuracy_manhattan_list, label=\"manhattan\")\n",
        "    plt.title('Plot of accuracy for model#2 with different distance function')\n",
        "    plt.xlabel('k (number of neighbours)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(range(0,21,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    "
      ],
      "metadata": {
        "id": "oMsg-tMNmZHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L-fold cross_validation for KNN"
      ],
      "metadata": {
        "id": "sQm6mXpvvrQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nISxfG7-k5hM"
      },
      "outputs": [],
      "source": [
        "def make_prediction(N, k, x, y):\n",
        "    model = KNN(K=k)\n",
        "    square_error = []\n",
        "    for i in range(30):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "        x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "        y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "        # To get hard predictions by choosing the class with the maximum probability\n",
        "        y_pred = np.argmax(y_prob, axis=-1)\n",
        "        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n",
        "        square_error.append(np.square(np.subtract(y_pred,y_test)).mean())\n",
        "    mean_square_error=sum(square_error) / len(square_error)\n",
        "    # print(f'accuracy is {accuracy * 100:.1f}.')\n",
        "    # print(f'mean squared error is {mean_square_error}.')\n",
        "    return mean_square_error\n",
        "\n",
        "\n",
        "def cross_validation(k, x, y):\n",
        "    model = KNN(K=k)\n",
        "    accuracy_list = []\n",
        "    square_error = []\n",
        "    for i in range(10):\n",
        "        split_x = np.split(x, 10)\n",
        "        val_x = split_x.pop(i)\n",
        "        train_x = np.concatenate(split_x)\n",
        "        split_y = np.array_split(y, 10)\n",
        "        val_y = split_y.pop(i)\n",
        "        train_y = np.concatenate(split_y)\n",
        "\n",
        "        y_prob, knns = model.fit(train_x, train_y).predict(val_x)\n",
        "        # To get hard predictions by choosing the class with the maximum probability\n",
        "        y_pred = np.argmax(y_prob, axis=-1)\n",
        "        square_error.append(np.square(np.subtract(y_pred,val_y)).mean())\n",
        "        accuracy_list.append(np.sum(y_pred == val_y) / val_y.shape[0])\n",
        "    mean_square_error = sum(square_error) / len(square_error)\n",
        "\n",
        "    accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "    # print(f'accuracy is {accuracy * 100:.1f}.')\n",
        "    # print(f'mean squared error is {mean_square_error}.')\n",
        "    return mean_square_error\n",
        "\n",
        "def get_average_accuracy_fold(N, i, x, y):\n",
        "    accuracy1 = []\n",
        "    accuracy2 = []\n",
        "    for k in range(30):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "        x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "        accuracy1.append(cross_validation(i, x_train, y_train))\n",
        "        accuracy2.append(make_prediction(N, i, x, y))\n",
        "    return sum(accuracy1) / len(accuracy1), sum(accuracy2) / len(accuracy2)\n",
        "\n",
        "def L_fold_with_plot_knn(x,y,N):\n",
        "    accuracy_for_test = []\n",
        "    accuracy_for_validation = []\n",
        "    k = []\n",
        "\n",
        "    for i in range(1, 30):\n",
        "        k.append(i)\n",
        "        acc1, acc2 = get_average_accuracy_fold(N, i, x, y)\n",
        "        accuracy_for_validation.append(acc1)\n",
        "        accuracy_for_test.append(acc2)\n",
        "    std = statistics.stdev(accuracy_for_validation)\n",
        "    plt.plot(k, accuracy_for_test, label=\"test\")\n",
        "    plt.errorbar(k, accuracy_for_validation, std, label=\"validation\")\n",
        "    plt.title('Plot of the mean and standard deviation in 10 fold cross-validation')\n",
        "    plt.xlabel('k (number of neighbours)')\n",
        "    plt.ylabel('mean squared error')\n",
        "    plt.xticks(range(0,30,5))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN: 2 Most Important Features and Decision Boundary"
      ],
      "metadata": {
        "id": "Zx-Zh_YbvmwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def two_most_important_feature(dataset2):\n",
        "  max_accuracy = 0\n",
        "  two_features = [None, None]\n",
        "  j = 1\n",
        "  max_j = 0\n",
        "  max_k = 0\n",
        "  while j<18:\n",
        "    k = j + 1\n",
        "    while k<=18:\n",
        "      y = dataset2.iloc[:, -1].to_numpy()\n",
        "      x1 = dataset2.iloc[:, j].to_numpy()\n",
        "      x2 = dataset2.iloc[:, k].to_numpy()\n",
        "      x = np.concatenate((x1[:,None], x2[:,None]), axis = 1)\n",
        "        \n",
        "      (N,D), C = x.shape, np.max(y)+1                                                    \n",
        "      #print(f'instances (N) \\t {N} \\nfeatures (D) \\t {D} \\nclasses (C) \\t {C}')\n",
        "      #deleted_column = df.pop(df.columns[0])\n",
        "\n",
        "      #print(\"The features that used are \" + dataset1.columns[j] + \" \" + dataset1.columns[k])\n",
        "      model = KNN(K=12)\n",
        "      accuracy_list=[]\n",
        "      inds = np.random.permutation(N)\n",
        "      accuracies = 0\n",
        "      for i in range(100):\n",
        "    \n",
        "        inds = np.random.permutation(N)  \n",
        "        #split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "        x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "        y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "\n",
        "        #To get hard predictions by choosing the class with the maximum probability\n",
        "        y_pred = np.argmax(y_prob,axis=-1)        \n",
        "        accuracy_list.append(np.sum(y_pred == y_test)/y_test.shape[0])\n",
        "\n",
        "        accuracy = sum(accuracy_list) /len(accuracy_list)\n",
        "        accuracies += accuracy\n",
        "      accuracies /= 100\n",
        "        #print(f'accuracy is {accuracies*100:.1f}.')\n",
        "      if(accuracies>max_accuracy):\n",
        "        max_accuracy = accuracies\n",
        "        two_features[0] = dataset2.columns[j]\n",
        "        two_features[1] = dataset2.columns[k]\n",
        "        max_j = j\n",
        "        max_k = k\n",
        "      k = k + 1\n",
        "    j = j + 1\n",
        "  print(f'accuracy is {max_accuracy*100:.1f}.')\n",
        "  print(\"The features that used are \" + two_features[0] + \" and \" + two_features[1])\n",
        "\n",
        "  y = dataset2.iloc[:, -1].to_numpy()\n",
        "  x_1 = dataset2.iloc[:, max_j].to_numpy()\n",
        "  x_2 = dataset2.iloc[:, max_k].to_numpy()\n",
        "  x = np.concatenate((x_1[:,None], x_2[:,None]), axis = 1)\n",
        "\n",
        "  (N,D), C = x.shape, np.max(y)+1\n",
        "  inds = np.random.permutation(N)\n",
        "  x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "  x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "\n",
        "  x0v = np.linspace(np.min(x[:, 0]), np.max(x[:, 0]), 200)\n",
        "  x1v = np.linspace(np.min(x[:, 1]), np.max(x[:, 1]), 200)\n",
        "  x0, x1 = np.meshgrid(x0v, x1v)\n",
        "  x_all = np.vstack((x0.ravel(), x1.ravel())).T\n",
        "\n",
        "  model = KNN(K=12)\n",
        "  y_train_prob = np.zeros((y_train.shape[0], C+1))\n",
        "\n",
        "  y_train_prob[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "  y_prob_all, _ = model.fit(x_train, y_train).predict(x_all)\n",
        "\n",
        "  y_pred_all = np.zeros((y_prob_all.shape[0], C+1))\n",
        "  y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1\n",
        "  print(y_pred_all)\n",
        "\n",
        "  plt.scatter(x_train[:,0], x_train[:,1], c=y_train_prob, marker='o', alpha=1)\n",
        "  plt.scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=0.01)\n",
        "  plt.ylabel(dataset2.columns[max_k])\n",
        "  plt.xlabel(dataset2.columns[max_j])\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "Pb5p1k8rvlXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree for different cost function"
      ],
      "metadata": {
        "id": "glRq6_4L3DRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "class Node:\n",
        "    def __init__(self, data_indices, parent):\n",
        "        self.data_indices = data_indices    # stores the data indices which are in the region defined by this node\n",
        "        self.left = None                    # stores the left child of the node\n",
        "        self.rigth = None                   # stores the right child of the node\n",
        "        self.split_feature = None           # the feature for split at this node\n",
        "        self.split_value = None             # the value of the feature for split at this node\n",
        "        if parent:\n",
        "            self.depth = parent.depth + 1   # obtain the depth of the node by adding one to depth of the parent\n",
        "            self.num_classes = parent.num_classes   # copies the num classes from the parent\n",
        "            self.data = parent.data         # copies the data from the parent\n",
        "            self.labels = parent.labels     # copies the labels from the parent\n",
        "            class_prob = np.bincount(self.labels[data_indices], minlength=self.num_classes)\n",
        "                        # this is counting frequency of different labels in the region defined by this node\n",
        "            self.class_prob = class_prob / np.sum(class_prob)\n",
        "                        # stores the class probability for the node\n",
        "            # we'll use the class probabilities of the leaf nodes for making predictions after the tree is built\n",
        "\n",
        "\n",
        "def greedy_test(node, cost_fn):\n",
        "    # initialize the best parameter values\n",
        "    best_cost = np.inf\n",
        "    best_feature, best_value = None, None\n",
        "    num_instances, num_features = node.data.shape\n",
        "            \n",
        "    #sort the features to get the test value candidates by taking\n",
        "    #the average of consecutive sorted feature values\n",
        "    data_sorted = np.sort(node.data[node.data_indices], axis=0)\n",
        "    test_candidates = (data_sorted[1:] + data_sorted[:-1]) / 2.\n",
        "    for f in range(num_features):\n",
        "        data_f = node.data[node.data_indices, f]\n",
        "        for test in test_candidates[:, f]:\n",
        "\n",
        "            #Split the indices using the test value of f-th feature\n",
        "            left_indices = node.data_indices[data_f <= test]\n",
        "            right_indices = node.data_indices[data_f > test]\n",
        "\n",
        "            #We can't have a split where a child has zero element\n",
        "            #if this is true over all the test features and their test values\n",
        "            #then the function returns the best coset as infinity\n",
        "            if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                continue\n",
        "            #compute the left and right cost based on the current split\n",
        "            left_cost = cost_fn(node.labels[left_indices])\n",
        "            right_cost = cost_fn(node.labels[right_indices])\n",
        "            num_left, num_right = left_indices.shape[0], right_indices.shape[0]\n",
        "\n",
        "            # get combined cost using the weighted sum of left and right cost\n",
        "            cost = (num_left * left_cost + num_right * right_cost) / num_instances\n",
        "\n",
        "            if cost < best_cost:\n",
        "                best_cost = cost\n",
        "                best_feature = f\n",
        "                best_value = test\n",
        "    return best_cost, best_feature, best_value\n",
        "\n",
        "def cost_misclassification(labels):\n",
        "    counts = np.bincount(labels)\n",
        "    class_probs = counts / np.sum(counts)\n",
        "    return 1 - np.max(class_probs)\n",
        "\n",
        "def cost_entropy(labels):\n",
        "    class_probs = np.bincount(labels) / len(labels)\n",
        "    class_probs = class_probs[class_probs > 0]\n",
        "    return -np.sum(class_probs * np.log(class_probs)) \n",
        "\n",
        "def cost_gini_index(labels):\n",
        "    class_probs = np.bincount(labels) / len(labels)\n",
        "    return 1 - np.sum(np.square(class_probs))\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, num_classes=None, max_depth=3, cost_fn=cost_misclassification, min_leaf_instances=1):\n",
        "        self.max_depth = max_depth      #maximum dept for termination \n",
        "        self.root = None                #stores the root of the decision tree \n",
        "        self.cost_fn = cost_fn          #stores the cost function of the decision tree \n",
        "        self.num_classes = num_classes  #stores the total number of classes\n",
        "        self.min_leaf_instances = min_leaf_instances  #minimum number of instances in a leaf for termination\n",
        "        \n",
        "    def fit(self, data, labels):\n",
        "        pass                            #pass in python 3 means nothing happens and the method here is empty\n",
        "    \n",
        "    def predict(self, data_test):\n",
        "        pass\n",
        "\n",
        "def fit(self, data, labels):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    if self.num_classes is None:\n",
        "        self.num_classes = np.max(labels) + 1\n",
        "    #below are initialization of the root of the decision tree\n",
        "    self.root = Node(np.arange(data.shape[0]), None)\n",
        "    self.root.data = data\n",
        "    self.root.labels = labels\n",
        "    self.root.num_classes = self.num_classes\n",
        "    self.root.depth = 0\n",
        "    #to recursively build the rest of the tree\n",
        "    self._fit_tree(self.root)\n",
        "    return self\n",
        "\n",
        "def _fit_tree(self, node):\n",
        "    #This gives the condition for termination of the recursion resulting in a leaf node\n",
        "    if node.depth == self.max_depth or len(node.data_indices) <= self.min_leaf_instances:\n",
        "        return\n",
        "    #greedily select the best test by minimizing the cost\n",
        "    cost, split_feature, split_value = greedy_test(node, self.cost_fn)\n",
        "    #if the cost returned is infinity it means that it is not possible to split the node and hence terminate\n",
        "    if np.isinf(cost):\n",
        "        return\n",
        "    #print(f'best feature: {split_feature}, value {split_value}, cost {cost}')\n",
        "    #to get a boolean array suggesting which data indices corresponding to this node are in the left of the split\n",
        "    test = node.data[node.data_indices,split_feature] <= split_value\n",
        "    #store the split feature and value of the node\n",
        "    node.split_feature = split_feature\n",
        "    node.split_value = split_value\n",
        "    #define new nodes which are going to be the left and right child of the present node\n",
        "    left = Node(node.data_indices[test], node)\n",
        "    right = Node(node.data_indices[np.logical_not(test)], node)\n",
        "    #recursive call to the _fit_tree()\n",
        "    self._fit_tree(left)\n",
        "    self._fit_tree(right)\n",
        "    #assign the left and right child to present child\n",
        "    node.left = left\n",
        "    node.right = right\n",
        "\n",
        "DecisionTree.fit = fit\n",
        "DecisionTree._fit_tree = _fit_tree\n",
        "\n",
        "def predict(self, data_test):\n",
        "    class_probs = np.zeros((data_test.shape[0], self.num_classes))\n",
        "    for n, x in enumerate(data_test):\n",
        "        node = self.root\n",
        "        #loop along the dept of the tree looking region where the present data sample fall in based on the split feature and value\n",
        "        while node.left:\n",
        "            if x[node.split_feature] <= node.split_value:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        #the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction\n",
        "        class_probs[n,:] = node.class_prob\n",
        "    return class_probs\n",
        "DecisionTree.predict = predict\n",
        "\n",
        "def make_prediction_depth(N, depth, x, y):\n",
        "    tree1 = DecisionTree(max_depth=depth)    #### using misclassification as cost function\n",
        "    tree2 = DecisionTree(max_depth=depth, cost_fn = cost_entropy) \n",
        "    tree3 = DecisionTree(max_depth=depth, cost_fn = cost_gini_index)\n",
        "    accuracy_list1=[]\n",
        "    accuracy_list2=[]\n",
        "    accuracy_list3 = []\n",
        "    for i in range(10):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "        x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "\n",
        "        #### using misclassification as cost function\n",
        "        probs_test1 = tree1.fit(x_train, y_train).predict(x_test)\n",
        "        probs_test2 = tree2.fit(x_train, y_train).predict(x_test)\n",
        "        probs_test3 = tree3.fit(x_train, y_train).predict(x_test)\n",
        "        y_pred1 = np.argmax(probs_test1,1)\n",
        "        y_pred2 = np.argmax(probs_test2,1)\n",
        "        y_pred3 = np.argmax(probs_test3,1)\n",
        "        accuracy_list1.append(np.sum(y_pred1 == y_test) / y_test.shape[0])\n",
        "        accuracy_list2.append(np.sum(y_pred2 == y_test) / y_test.shape[0])\n",
        "        accuracy_list3.append(np.sum(y_pred3 == y_test) / y_test.shape[0])\n",
        "\n",
        "    accuracy1 = sum(accuracy_list1) / len(accuracy_list1)\n",
        "    accuracy2 = sum(accuracy_list2) / len(accuracy_list2)\n",
        "    accuracy3 = sum(accuracy_list3) / len(accuracy_list3)\n",
        "    \n",
        "    return accuracy1,accuracy2,accuracy3\n",
        "\n",
        "def compare_diff_cost_func(x,y,N):\n",
        "    depth_list=[]\n",
        "    accuracy_misclassification_list=[]\n",
        "    accuracy_entropy_list=[]\n",
        "    accuracy_gini_index_list=[]\n",
        "    for k in range(1,11):\n",
        "        #print(k)\n",
        "        accuracy_misclassification, accuracy_entropy, accuracy_gini_index=make_prediction_depth(N, k, x, y)\n",
        "        accuracy_misclassification_list.append(accuracy_misclassification)\n",
        "        accuracy_entropy_list.append(accuracy_entropy)\n",
        "        accuracy_gini_index_list.append(accuracy_gini_index)\n",
        "        depth_list.append(k)\n",
        "    plt.plot(depth_list, accuracy_misclassification_list, label=\"misclassification\")\n",
        "    plt.plot(depth_list, accuracy_entropy_list, label=\"entropy\")\n",
        "    plt.plot(depth_list, accuracy_gini_index_list, label=\"gini_index\")\n",
        "    plt.title('Plot of accuracy for model#2 with different cost function')\n",
        "    plt.xlabel('Tree Depth')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(range(0,11,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "def model_tree_for_specific_depth(x,y,N,depth):\n",
        "    for k in range(depth,depth+1):\n",
        "        accuracy_misclassification, accuracy_entropy, accuracy_gini_index=make_prediction_depth(N, k, x, y)\n",
        "        print('the average accuracy of decision tree model with misclassification cost for tree depth ',depth,f' is {accuracy_misclassification*100:.1f}.')\n",
        "        print('the average accuracy of decision tree model with entropy cost for tree depth ',depth,f' is {accuracy_entropy*100:.1f}.')\n",
        "        print('the average accuracy of decision tree model with gini cost for tree depth ',depth,f' is {accuracy_gini_index*100:.1f}.')\n",
        "\n"
      ],
      "metadata": {
        "id": "pGlsxfKB3Ccp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L-fold Cross_validation for Decision Tree"
      ],
      "metadata": {
        "id": "qqUEbQyMwqVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(N, depth, x, y):\n",
        "    tree1 = DecisionTree(max_depth=depth)    #### using misclassification as cost function\n",
        "    tree2 = DecisionTree(max_depth=depth, cost_fn = cost_entropy) \n",
        "    tree3 = DecisionTree(max_depth=depth, cost_fn = cost_gini_index)\n",
        "    \n",
        "    square_error = []\n",
        "    accuracy_list = []\n",
        "    for i in range(10):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "        x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "\n",
        "        #### using misclassification as cost function\n",
        "        probs_test = tree3.fit(x_train, y_train).predict(x_test)\n",
        "        y_pred = np.argmax(probs_test,1)\n",
        "        \n",
        "        #print(f'accuracy using gini index as cost function is {accuracy*100:.1f}.')\n",
        "        accuracy_list.append(np.sum(y_pred == y_test) / y_test.shape[0])\n",
        "        square_error.append(np.square(np.subtract(y_pred,y_test)).mean())\n",
        "    mean_square_error = sum(square_error) / len(square_error)\n",
        "    accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "    # print(f'accuracy is {accuracy * 100:.1f}.')\n",
        "    # print(f'mean squared error is {mean_square_error}.')\n",
        "    return mean_square_error\n",
        "\n",
        "\n",
        "def cross_validation(depth, x, y):\n",
        "    tree1 = DecisionTree(max_depth=depth) \n",
        "    tree2 = DecisionTree(max_depth=depth, cost_fn = cost_entropy) \n",
        "    tree3 = DecisionTree(max_depth=depth, cost_fn = cost_gini_index)\n",
        "    accuracy_list = []\n",
        "    square_error = []\n",
        "    for i in range(10):\n",
        "        split_x = np.split(x, 10)\n",
        "        val_x = split_x.pop(i)\n",
        "\n",
        "        train_x = np.concatenate(split_x)\n",
        "\n",
        "        split_y = np.array_split(y, 10)\n",
        "        val_y = split_y.pop(i)\n",
        "        train_y = np.concatenate(split_y)\n",
        "\n",
        "        probs_test = tree3.fit(train_x, train_y).predict(val_x)\n",
        "        # To get hard predictions by choosing the class with the maximum probability\n",
        "        y_pred = np.argmax(probs_test, axis=-1)\n",
        "        square_error.append(np.square(np.subtract(y_pred,val_y)).mean())\n",
        "        accuracy_list.append(np.sum(y_pred == val_y) / val_y.shape[0])\n",
        "    mean_square_error = sum(square_error) / len(square_error)\n",
        "    accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "    # print(f'accuracy is {accuracy * 100:.1f}.')\n",
        "    # print(f'mean squared error is {mean_square_error}.')\n",
        "    return mean_square_error\n",
        "\n",
        "\n",
        "def maximum_absolute_scaling(df):\n",
        "    # copy the dataframe\n",
        "    df_scaled = df.copy()\n",
        "    # scale_col = [\"AGE\", \"BILIRUBIN\", \"ALK PHOSPHATE\", \"SGOT\", \"ALBUMIN\", \"PROTIME\"]\n",
        "    # apply maximum absolute scaling\n",
        "    df_scaled.iloc[:, 2:18] = df_scaled.iloc[:, 2:18] / df_scaled.iloc[:, 2:18].abs().max()\n",
        "    return df_scaled\n",
        "\n",
        "\n",
        "def get_average_accuracy_fold(N, i, x, y):\n",
        "    accuracy1 = []\n",
        "    accuracy2 = []\n",
        "    for k in range(5):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "        x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "        accuracy1.append(cross_validation(i, x_train, y_train))\n",
        "        accuracy2.append(make_prediction(N, i, x, y))\n",
        "    return sum(accuracy1) / len(accuracy1), sum(accuracy2) / len(accuracy2)\n",
        "\n",
        "def L_fold_with_plot_tree(x,y,N):\n",
        "    accuracy_for_test = []\n",
        "    accuracy_for_validation = []\n",
        "    tree_depth = []\n",
        "\n",
        "    for i in range(1,15):\n",
        "        tree_depth.append(i)\n",
        "        acc1, acc2 = get_average_accuracy_fold(N,i, x, y)\n",
        "        accuracy_for_validation.append(acc1)\n",
        "        accuracy_for_test.append(acc2)\n",
        "    std = statistics.stdev(accuracy_for_validation)\n",
        "    plt.plot(tree_depth, accuracy_for_test, label=\"test\")\n",
        "    plt.errorbar(tree_depth, accuracy_for_validation, std, label=\"validation\")\n",
        "    plt.title('Plot of the mean and standard deviation in 10 fold cross-validation')\n",
        "    plt.xlabel('Tree depth')\n",
        "    plt.ylabel('mean squared error')\n",
        "    plt.xticks(range(0,15,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Y8ERcVpLwqBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrb3IqkyB1Nv"
      },
      "source": [
        "# remove feature Exudate2 and MA-1.0 accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Diel0lOY7vd"
      },
      "outputs": [],
      "source": [
        "def remove_two_features_accuracy(dataset2):\n",
        "  y = dataset2.iloc[:, -1].to_numpy()\n",
        "  x1 = dataset2.iloc[:, 0:7].to_numpy()\n",
        "  x2 = dataset2.iloc[:, 8:9].to_numpy()\n",
        "  x3 = dataset2.iloc[:, 10:-1].to_numpy()\n",
        "  x = np.concatenate((x1, x2, x3), axis=1)\n",
        "      \n",
        "  (N,D), C = x.shape, np.max(y)+1                                                    \n",
        "  print(f'instances (N) \\t {N} \\nfeatures (D) \\t {D} \\nclasses (C) \\t {C}')\n",
        "    #deleted_column = df.pop(df.columns[0])\n",
        "\n",
        "  print(\"The features that skipped is \", dataset2.columns[7], \" and \", dataset2.columns[9])\n",
        "  model = KNN(K=13)\n",
        "  accuracy_list=[]\n",
        "  inds = np.random.permutation(N)\n",
        "  accuracies = 0\n",
        "  for i in range(500):\n",
        "    \n",
        "    inds = np.random.permutation(N)  \n",
        "    #split the dataset into train and test(80% training & 20% testing)\n",
        "    x_train, y_train = x[inds[:920]], y[inds[:920]]\n",
        "    x_test, y_test = x[inds[920:]], y[inds[920:]]\n",
        "    y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "\n",
        "    #To get hard predictions by choosing the class with the maximum probability\n",
        "    y_pred = np.argmax(y_prob,axis=-1)        \n",
        "    accuracy_list.append(np.sum(y_pred == y_test)/y_test.shape[0])\n",
        "\n",
        "    accuracy = sum(accuracy_list) /len(accuracy_list)\n",
        "    accuracies += accuracy\n",
        "  accuracies /= 500\n",
        "  print(f'accuracy after skipping feature Exudate2 and MA-1.0 is {accuracies*100:.1f}.')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "zPlpFpm7da-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    #loading and pre-clean the raw data\n",
        "    dataset2=load_and_clean_data()\n",
        "\n",
        "    #scaled data into range [0,1] and split the set to feature set x and label set y\n",
        "    #x,y,N=scaled_data(dataset2)\n",
        "\n",
        "    #data without scale and split the set to feature set x and label set y\n",
        "    x,y,N=unscaled_data(dataset2)\n",
        "\n",
        "    #print the accuracy of knn model with specific num of neighbor\n",
        "    #model_knn_for_specific_k(x,y,N,13)\n",
        "\n",
        "    #generate a plot that shows the accuracy of model with different distance function \n",
        "    #compare_diff_distance_func(x,y,N)\n",
        "\n",
        "    #implement the 10-fold cross validation of knn model\n",
        "    #L_fold_with_plot_knn(x,y,N)\n",
        "\n",
        "    #function for KNN that finds two most important features for \n",
        "    #two_most_important_feature(dataset2)\n",
        "\n",
        "    #print the accuracy of decision tree model with specific tree depth\n",
        "    #model_tree_for_specific_depth(x,y,N,2)\n",
        "\n",
        "    #implement the 10-fold cross validation of decision tree model\n",
        "    #L_fold_with_plot_tree(x,y,N)\n",
        "\n",
        "    #generate a plot that shows the accuracy of model with different cost function \n",
        "    #compare_diff_cost_func(x,y,N)\n",
        "\n",
        "    #This function skip the feature MA-1.0 and Exudate-2 and find the accuracy\n",
        "    #remove_two_features_accuracy(dataset2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPZe9b5kdcEL",
        "outputId": "1e7d3fc1-c65c-45a4-a1da-41cec3c3a93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instances (N) \t 1151 \n",
            "features (D) \t 19 \n",
            "classes (C) \t 2\n",
            "instances (N) \t 1151 \n",
            "features (D) \t 19 \n",
            "classes (C) \t 2\n",
            "instances (N) \t 1151 \n",
            "features (D) \t 17 \n",
            "classes (C) \t 2\n",
            "The features that skipped is  MA-1.0  and  Exudates-2\n",
            "accuracy after skipping feature Exudate2 and MA-1.0 is 67.4.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "y2iCK8NtmYo5",
        "sQm6mXpvvrQ9",
        "Zx-Zh_YbvmwC",
        "glRq6_4L3DRC",
        "qqUEbQyMwqVe"
      ],
      "name": "MiniProject1_Diabrtic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}