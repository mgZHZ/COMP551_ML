{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmYsvUKzNQFX"
      },
      "source": [
        "# Initialize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpN4em4C08VJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "from google.colab import files\n",
        "#upload the file from local\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJnIbPwqYBj1"
      },
      "source": [
        "# define KNN class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5IdI0mo1ek_"
      },
      "outputs": [],
      "source": [
        "#define different distance function\n",
        "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2,axis = -1))\n",
        "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
        "\n",
        "class KNN:\n",
        "\n",
        "    def __init__(self, K, dist_fn= euclidean):\n",
        "        self.dist_fn = dist_fn\n",
        "        self.K = K\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.C = np.max(y)+1\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x_test):\n",
        "        num_test = x_test.shape[0]\n",
        "        #calculate distance between the training & test samples and returns an array of shape [num_test, num_train]    \n",
        "        distances = self.dist_fn(self.x[None, :, :], x_test[:, None, :])\n",
        "        #ith-row of knns stores the indices of k closest training samples to the ith-test sample \n",
        "        knns = np.zeros((num_test, self.K), dtype=int)\n",
        "        #ith-row of y_prob has the probability distribution over C classes\n",
        "        y_prob = np.zeros((num_test, self.C))\n",
        "        for i in range(num_test):\n",
        "            #find k nearest neighbor\n",
        "            knns[i,:] = np.argsort(distances[i])[:self.K]  \n",
        "            #counts the number of instances of each class in the K-closest training samples\n",
        "            y_prob[i,:] = np.bincount(self.y[knns[i,:]], minlength=self.C)\n",
        "        #divide by K to get the probability distribution\n",
        "        y_prob /= self.K\n",
        "        return y_prob, knns\n",
        "\n",
        "#load and clean the raw data\n",
        "def load_and_clean_data():\n",
        "    #load the dataset hepatitis.data\n",
        "    dataset1 = pd.read_csv('hepatitis.data')\n",
        "    dataset1.columns = ['Class', 'AGE', 'SEX', 'STEROID', 'ANTIVIRALS', 'FATIGUE', 'MALAISE', 'ANOREXIA', 'LIVER BIG', 'LIVER FIRM', 'SPLEEN PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES', 'BILIRUBIN', 'ALK PHOSPHATE', 'SGOT', 'ALBUMIN', 'PROTIME', 'HISTOLOGY']\n",
        "    #drop the row with any missing value\n",
        "    dataset1=dataset1.replace(\"?\",np.nan).dropna()\n",
        "    #reset the index of row\n",
        "    dataset1.reset_index(inplace=True)\n",
        "    dataset1=dataset1.drop(columns=\"index\")\n",
        "    #change all the data to numeric \n",
        "    dataset1 = dataset1.apply(pd.to_numeric)\n",
        "    #scale the value of Class to [0,1]\n",
        "    dataset1.Class=dataset1.Class-1\n",
        "    #print the dataset\n",
        "    print(dataset1)\n",
        "    return dataset1\n",
        "\n",
        "#scale all feature columns to range [0,1]\n",
        "def maximum_absolute_scaling(df):\n",
        "    #make a copy of the dataframe\n",
        "    df_scaled = df.copy()\n",
        "    #apply maximum absolute scaling on all non_binary features\n",
        "    scale_col = [\"AGE\", \"BILIRUBIN\", \"ALK PHOSPHATE\", \"SGOT\", \"ALBUMIN\", \"PROTIME\"]\n",
        "    for column in scale_col:\n",
        "        df_scaled[column] = df_scaled[column] / df_scaled[column].abs().max()+1\n",
        "    df_scaled.iloc[:,1:] = df_scaled.iloc[:,1:] -1\n",
        "    return df_scaled\n",
        "\n",
        "#scale and split the data to: set x (stores features), set y(stores lable)\n",
        "def scaled_data(dataset1):\n",
        "    # call the maximum_absolute_scaling function to scale features\n",
        "    df_scaled = maximum_absolute_scaling(dataset1)\n",
        "    y, x = df_scaled.iloc[:, 0].to_numpy(), df_scaled.iloc[:, 1:].to_numpy()\n",
        "    (N, D), C = x.shape, np.max(y) + 1\n",
        "    print(df_scaled)\n",
        "    print(f'instances (N) \\t {N} \\nfeatures (D) \\t {D} \\nclasses (C) \\t {C}')\n",
        "    return x,y,N\n",
        "\n",
        "#split the data to: set x (stores features), set y(stores lable)\n",
        "def unscaled_data(dataset1):\n",
        "    y, x = dataset1.iloc[:, 0].to_numpy(), dataset1.iloc[:, 1:].to_numpy()\n",
        "    (N, D), C = x.shape, np.max(y) + 1\n",
        "    print(f'instances (N) \\t {N} \\nfeatures (D) \\t {D} \\nclasses (C) \\t {C}')\n",
        "    return x,y,N\n",
        "\n",
        "#print the accuracy of knn model with specific k neighbor\n",
        "def model_knn_for_specific_k(x, y, N, k):\n",
        "    model = KNN(K=k)\n",
        "    accuracy_list=[]\n",
        "    for i in range(500):\n",
        "      inds = np.random.permutation(N)  \n",
        "      #split the dataset into train and test(80% training & 20% testing)\n",
        "      x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "      x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "      y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "      #get predictions by choosing the class with the maximum probability\n",
        "      y_pred = np.argmax(y_prob,axis=-1)        \n",
        "      accuracy_list.append(np.sum(y_pred == y_test)/y_test.shape[0])\n",
        "\n",
        "    accuracy=sum(accuracy_list) /len(accuracy_list)\n",
        "    print('knns shape:', knns.shape)\n",
        "    print('y_prob shape:', y_prob.shape)\n",
        "    print('the average accuracy for k is',k,f'is {accuracy*100:.1f}.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBIyycfbbHyb"
      },
      "source": [
        "# KNN with different distance function - plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SndSP6sdbHSz"
      },
      "outputs": [],
      "source": [
        "#compute the average accuracy of test set with k neighbors and different distance functions\n",
        "def make_prediction_dist(N, k, x, y):\n",
        "    model1 = KNN(K=k,dist_fn= euclidean)\n",
        "    model2 = KNN(K=k,dist_fn= manhattan)\n",
        "    accuracy_list1=[]\n",
        "    accuracy_list2=[]\n",
        "    for i in range(100):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "        x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "        y_prob, knns = model1.fit(x_train, y_train).predict(x_test)\n",
        "        y_prob1, knns1 = model2.fit(x_train, y_train).predict(x_test)\n",
        "        # get predictions by choosing the class with the maximum probability\n",
        "        y_pred = np.argmax(y_prob, axis=-1)\n",
        "        y_pred1 = np.argmax(y_prob1, axis=-1)\n",
        "        accuracy_list1.append(np.sum(y_pred == y_test) / y_test.shape[0])\n",
        "        accuracy_list2.append(np.sum(y_pred1 == y_test) / y_test.shape[0])\n",
        "    accuracy_euclidean=sum(accuracy_list1) /len(accuracy_list1)\n",
        "    accuracy_manhattan=sum(accuracy_list2) /len(accuracy_list2)\n",
        "    return accuracy_euclidean, accuracy_manhattan\n",
        "\n",
        "##generate a plot that shows the accuracy of model with different distance function\n",
        "def compare_diff_distance_func(x,y,N):\n",
        "    k_list=[]\n",
        "    accuracy_euclidean_list=[]\n",
        "    accuracy_manhattan_list=[]\n",
        "    # calculate the average accuracy of KNN model with different distance function for k=1,..,20\n",
        "    for k in range(1,21):\n",
        "        accuracy_euclidean, accuracy_manhattan=make_prediction_dist(N, k, x, y)\n",
        "        accuracy_euclidean_list.append(accuracy_euclidean)\n",
        "        accuracy_manhattan_list.append(accuracy_manhattan)\n",
        "        k_list.append(k)\n",
        "    plt.plot(k_list, accuracy_euclidean_list, label=\"euclidean\")\n",
        "    plt.plot(k_list, accuracy_manhattan_list, label=\"manhattan\")\n",
        "    plt.title('Plot of accuracy for model#1 with different distance function')\n",
        "    plt.xlabel('k (number of neighbours)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(range(0,21,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnEq1entohFt"
      },
      "source": [
        "# L-fold cross_validation for KNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wVgnDPFohpu"
      },
      "outputs": [],
      "source": [
        "#compute the average average mean squared error of test set with k neighbors(with default distance function)\n",
        "def make_prediction_knn(N, k, x, y):\n",
        "    model = KNN(K=k)\n",
        "    square_error = []\n",
        "    for i in range(50):\n",
        "        inds = np.random.permutation(N)\n",
        "        #split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "        x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "        y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "        #get predictions by choosing the class with the maximum probability\n",
        "        y_pred = np.argmax(y_prob, axis=-1)\n",
        "        #accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n",
        "        square_error.append(np.square(np.subtract(y_pred,y_test)).mean())\n",
        "    mean_square_error=sum(square_error) / len(square_error)\n",
        "    # print(f'accuracy is {accuracy * 100:.1f}.')\n",
        "    # print(f'mean squared error is {mean_square_error}.')\n",
        "    return mean_square_error\n",
        "\n",
        "#compute the mean squared error of validation set with k neighbors(with default distance function)\n",
        "def cross_validation_knn(k, x, y):\n",
        "    model = KNN(K=k)\n",
        "    #accuracy_list = []\n",
        "    square_error = []\n",
        "    #8-fold\n",
        "    for i in range(8):\n",
        "        split_x = np.split(x, 8)\n",
        "        #x for validation set\n",
        "        val_x = split_x.pop(i)\n",
        "        #x for tain set\n",
        "        train_x = np.concatenate(split_x)\n",
        "        split_y = np.array_split(y, 8)\n",
        "        #y for validation set\n",
        "        val_y = split_y.pop(i)\n",
        "        #y for tain set\n",
        "        train_y = np.concatenate(split_y)\n",
        "        y_prob, knns = model.fit(train_x, train_y).predict(val_x)\n",
        "        #get predictions by choosing the class with the maximum probability\n",
        "        y_pred = np.argmax(y_prob, axis=-1)\n",
        "        #calculate the mean squared error for model\n",
        "        square_error.append(np.square(np.subtract(y_pred,val_y)).mean())\n",
        "        #accuracy_list.append(np.sum(y_pred == val_y) / val_y.shape[0])\n",
        "    mean_square_error = sum(square_error) / len(square_error)\n",
        "    #accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "    return mean_square_error\n",
        "\n",
        "#compute the average mean squared error for validation set and training set\n",
        "def get_average_accuracy_fold_knn(N, i, x, y):\n",
        "    mse1 = []\n",
        "    mse2 = []\n",
        "    for k in range(50):\n",
        "        inds = np.random.permutation(N)\n",
        "        x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "        x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "        mse1.append(cross_validation_knn(i, x_train, y_train))\n",
        "        mse2.append(make_prediction_knn(N, i, x, y))\n",
        "    return sum(mse1) / len(mse1), sum(mse2) / len(mse2)\n",
        "\n",
        "#implement the 8-fold cross validation of knn model\n",
        "def L_fold_with_plot_knn(x,y,N):\n",
        "    mse_for_test = []\n",
        "    mse_for_validation = []\n",
        "    k = []\n",
        "    for i in range(1,30):\n",
        "        k.append(i)\n",
        "        mse1, mse2 = get_average_accuracy_fold_knn(N,i, x, y)\n",
        "        mse_for_validation.append(mse1)\n",
        "        mse_for_test.append(mse2)\n",
        "    std = statistics.stdev(mse_for_validation)\n",
        "    plt.plot(k, mse_for_test, label=\"test\")\n",
        "    plt.errorbar(k, mse_for_validation, std, label=\"validation\")\n",
        "    plt.title('Plot of the mean and standard deviation in 8 fold cross-validation')\n",
        "    plt.xlabel('k (number of neighbours)')\n",
        "    plt.ylabel('mean squared error')\n",
        "    plt.xticks(range(0,30,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPqaF8wZ40Q2"
      },
      "source": [
        "# define Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAzTb49d40pk"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, data_indices, parent):\n",
        "        # stores the data indices which are in the region defined by this node\n",
        "        self.data_indices = data_indices    \n",
        "        # stores the left child of the node\n",
        "        self.left = None                    \n",
        "        # stores the right child of the node\n",
        "        self.rigth = None                   \n",
        "        # the feature for split at this node\n",
        "        self.split_feature = None    \n",
        "        # the value of the feature for split at this node       \n",
        "        self.split_value = None             \n",
        "        if parent:\n",
        "            # obtain the depth of the node by adding one to depth of the parent\n",
        "            self.depth = parent.depth + 1 \n",
        "            # copies the num classes from the parent  \n",
        "            self.num_classes = parent.num_classes   \n",
        "            # copies the data from the parent\n",
        "            self.data = parent.data  \n",
        "            # copies the labels from the parent       \n",
        "            self.labels = parent.labels     \n",
        "            # count frequency of different labels in the region defined by this node\n",
        "            class_prob = np.bincount(self.labels[data_indices], minlength=self.num_classes)\n",
        "            # stores the class probability for the node\n",
        "            self.class_prob = class_prob / np.sum(class_prob)\n",
        "\n",
        "def greedy_test(node, cost_fn):\n",
        "    # initialize the best parameter values\n",
        "    best_cost = np.inf\n",
        "    best_feature, best_value = None, None\n",
        "    num_instances, num_features = node.data.shape\n",
        "            \n",
        "    #sort the features to get the test value candidates by taking\n",
        "    #the average of consecutive sorted feature values\n",
        "    data_sorted = np.sort(node.data[node.data_indices], axis=0)\n",
        "    test_candidates = (data_sorted[1:] + data_sorted[:-1]) / 2.\n",
        "    for f in range(num_features):\n",
        "        data_f = node.data[node.data_indices, f]\n",
        "        for test in test_candidates[:, f]:\n",
        "\n",
        "            #Split the indices using the test value of f-th feature\n",
        "            left_indices = node.data_indices[data_f <= test]\n",
        "            right_indices = node.data_indices[data_f > test]\n",
        "\n",
        "            #We can't have a split where a child has zero element\n",
        "            #if this is true over all the test features and their test values\n",
        "            #then the function returns the best coset as infinity\n",
        "            if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                continue\n",
        "            #compute the left and right cost based on the current split\n",
        "            left_cost = cost_fn(node.labels[left_indices])\n",
        "            right_cost = cost_fn(node.labels[right_indices])\n",
        "            num_left, num_right = left_indices.shape[0], right_indices.shape[0]\n",
        "\n",
        "            # get combined cost using the weighted sum of left and right cost\n",
        "            cost = (num_left * left_cost + num_right * right_cost) / num_instances\n",
        "\n",
        "            if cost < best_cost:\n",
        "                best_cost = cost\n",
        "                best_feature = f\n",
        "                best_value = test\n",
        "    return best_cost, best_feature, best_value\n",
        "\n",
        "#3 different cost function\n",
        "def cost_misclassification(labels):\n",
        "    counts = np.bincount(labels)\n",
        "    class_probs = counts / np.sum(counts)\n",
        "    return 1 - np.max(class_probs)\n",
        "\n",
        "def cost_entropy(labels):\n",
        "    class_probs = np.bincount(labels) / len(labels)\n",
        "    class_probs = class_probs[class_probs > 0]\n",
        "    return -np.sum(class_probs * np.log(class_probs)) \n",
        "\n",
        "def cost_gini_index(labels):\n",
        "    class_probs = np.bincount(labels) / len(labels)\n",
        "    return 1 - np.sum(np.square(class_probs))\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, num_classes=None, max_depth=3, cost_fn=cost_misclassification, min_leaf_instances=1):\n",
        "        self.max_depth = max_depth      \n",
        "        #stores the root of the decision tree \n",
        "        self.root = None              \n",
        "        #stores the cost function of the decision tree   \n",
        "        self.cost_fn = cost_fn \n",
        "        #stores the total number of classes         \n",
        "        self.num_classes = num_classes  \n",
        "        #minimum number of instances in a leaf for termination\n",
        "        self.min_leaf_instances = min_leaf_instances  \n",
        "        \n",
        "    def fit(self, data, labels):\n",
        "        pass                            \n",
        "    \n",
        "    def predict(self, data_test):\n",
        "        pass\n",
        "\n",
        "def fit(self, data, labels):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    if self.num_classes is None:\n",
        "        self.num_classes = np.max(labels) + 1\n",
        "    #below are initialization of the root of the decision tree\n",
        "    self.root = Node(np.arange(data.shape[0]), None)\n",
        "    self.root.data = data\n",
        "    self.root.labels = labels\n",
        "    self.root.num_classes = self.num_classes\n",
        "    self.root.depth = 0\n",
        "    #to recursively build the rest of the tree\n",
        "    self._fit_tree(self.root)\n",
        "    return self\n",
        "\n",
        "def _fit_tree(self, node):\n",
        "    #This gives the condition for termination of the recursion resulting in a leaf node\n",
        "    if node.depth == self.max_depth or len(node.data_indices) <= self.min_leaf_instances:\n",
        "        return\n",
        "    #greedily select the best test by minimizing the cost\n",
        "    cost, split_feature, split_value = greedy_test(node, self.cost_fn)\n",
        "    #if the cost returned is infinity it means that it is not possible to split the node and hence terminate\n",
        "    if np.isinf(cost):\n",
        "        return\n",
        "    #get a boolean array suggesting which data indices corresponding to this node are in the left of the split\n",
        "    test = node.data[node.data_indices,split_feature] <= split_value\n",
        "    #store the split feature and value of the node\n",
        "    node.split_feature = split_feature\n",
        "    node.split_value = split_value\n",
        "    #define new nodes which are going to be the left and right child of the present node\n",
        "    left = Node(node.data_indices[test], node)\n",
        "    right = Node(node.data_indices[np.logical_not(test)], node)\n",
        "    #recursive call to the _fit_tree()\n",
        "    self._fit_tree(left)\n",
        "    self._fit_tree(right)\n",
        "    #assign the left and right child to present child\n",
        "    node.left = left\n",
        "    node.right = right\n",
        "\n",
        "DecisionTree.fit = fit\n",
        "DecisionTree._fit_tree = _fit_tree\n",
        "\n",
        "def predict(self, data_test):\n",
        "    class_probs = np.zeros((data_test.shape[0], self.num_classes))\n",
        "    for n, x in enumerate(data_test):\n",
        "        node = self.root\n",
        "        #loop along the dept of the tree looking region where the present data sample fall in based on the split feature and value\n",
        "        while node.left:\n",
        "            if x[node.split_feature] <= node.split_value:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        #the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction\n",
        "        class_probs[n,:] = node.class_prob\n",
        "    return class_probs\n",
        "DecisionTree.predict = predict\n",
        "\n",
        "#compute the average accuracy of test set with specific depth(with different cost function)\n",
        "def make_prediction_depth(N, depth, x, y):\n",
        "    tree1 = DecisionTree(max_depth=depth)    \n",
        "    tree2 = DecisionTree(max_depth=depth, cost_fn = cost_entropy) \n",
        "    tree3 = DecisionTree(max_depth=depth, cost_fn = cost_gini_index)\n",
        "    accuracy_list1=[]\n",
        "    accuracy_list2=[]\n",
        "    accuracy_list3 = []\n",
        "    for i in range(50):\n",
        "        inds = np.random.permutation(N)\n",
        "        x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "        x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "        probs_test1 = tree1.fit(x_train, y_train).predict(x_test)\n",
        "        probs_test2 = tree2.fit(x_train, y_train).predict(x_test)\n",
        "        probs_test3 = tree3.fit(x_train, y_train).predict(x_test)\n",
        "        y_pred1 = np.argmax(probs_test1,1)\n",
        "        y_pred2 = np.argmax(probs_test2,1)\n",
        "        y_pred3 = np.argmax(probs_test3,1)\n",
        "        accuracy_list1.append(np.sum(y_pred1 == y_test) / y_test.shape[0])\n",
        "        accuracy_list2.append(np.sum(y_pred2 == y_test) / y_test.shape[0])\n",
        "        accuracy_list3.append(np.sum(y_pred3 == y_test) / y_test.shape[0])\n",
        "\n",
        "    accuracy1 = sum(accuracy_list1) / len(accuracy_list1)\n",
        "    accuracy2 = sum(accuracy_list2) / len(accuracy_list2)\n",
        "    accuracy3 = sum(accuracy_list3) / len(accuracy_list3)\n",
        "    \n",
        "    return accuracy1,accuracy2,accuracy3\n",
        "\n",
        "#print the accuracy of decision tree model with specific tree depth\n",
        "def model_tree_for_specific_depth(x,y,N,depth):\n",
        "    for k in range(depth,depth+1):\n",
        "        accuracy_misclassification, accuracy_entropy, accuracy_gini_index=make_prediction_depth(N, k, x, y)\n",
        "        print('the average accuracy of decision tree model with misclassification cost for tree depth ',depth,f' is {accuracy_misclassification*100:.1f}.')\n",
        "        print('the average accuracy of decision tree model with entropy cost for tree depth ',depth,f' is {accuracy_entropy*100:.1f}.')\n",
        "        print('the average accuracy of decision tree model with gini cost for tree depth ',depth,f' is {accuracy_gini_index*100:.1f}.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBBbt1lQgk6-"
      },
      "source": [
        "# L-fold cross_validation for Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZjlpNZlgjVA"
      },
      "outputs": [],
      "source": [
        "def make_prediction_tree(N, depth, x, y):\n",
        "    tree1 = DecisionTree(max_depth=depth)   \n",
        "    tree2 = DecisionTree(max_depth=depth, cost_fn = cost_entropy) \n",
        "    tree3 = DecisionTree(max_depth=depth, cost_fn = cost_gini_index)\n",
        "    square_error = []\n",
        "    accuracy_list = []\n",
        "    for i in range(10):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "        x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "        #fit with decision tree model with gini index function\n",
        "        probs_test = tree3.fit(x_train, y_train).predict(x_test)\n",
        "        y_pred = np.argmax(probs_test,1)\n",
        "        #calculate the accuracy/MSE\n",
        "        accuracy_list.append(np.sum(y_pred == y_test) / y_test.shape[0])\n",
        "        square_error.append(np.square(np.subtract(y_pred,y_test)).mean())\n",
        "    #get average accuracy/MSE\n",
        "    mean_square_error = sum(square_error) / len(square_error)\n",
        "    accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "    return mean_square_error\n",
        "\n",
        "# 8-fold, get the mean squared error on validation set with decision tree model\n",
        "def cross_validation_tree(depth, x, y):\n",
        "    tree1 = DecisionTree(max_depth=depth) \n",
        "    tree2 = DecisionTree(max_depth=depth, cost_fn = cost_entropy) \n",
        "    tree3 = DecisionTree(max_depth=depth, cost_fn = cost_gini_index)\n",
        "    accuracy_list = []\n",
        "    square_error = []\n",
        "    # 8-fold cross validation\n",
        "    for i in range(8):\n",
        "        split_x = np.split(x, 8)\n",
        "        #x for validation set\n",
        "        val_x = split_x.pop(i)\n",
        "        #x for training set\n",
        "        train_x = np.concatenate(split_x)\n",
        "        split_y = np.array_split(y, 8)\n",
        "        #y for validation set\n",
        "        val_y = split_y.pop(i)\n",
        "        #y for training set\n",
        "        train_y = np.concatenate(split_y)\n",
        "        probs_test = tree3.fit(train_x, train_y).predict(val_x)\n",
        "        y_pred = np.argmax(probs_test, axis=-1)\n",
        "        square_error.append(np.square(np.subtract(y_pred,val_y)).mean())\n",
        "        #accuracy_list.append(np.sum(y_pred == val_y) / val_y.shape[0])\n",
        "    mean_square_error = sum(square_error) / len(square_error)\n",
        "    #accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "    return mean_square_error\n",
        "\n",
        "def get_average_accuracy_fold_tree(N, i, x, y):\n",
        "    mse1 = []\n",
        "    mse2 = []\n",
        "    for k in range(30):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "        x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "        mse1.append(cross_validation_tree(i, x_train, y_train))\n",
        "        mse2.append(make_prediction_tree(N, i, x, y))\n",
        "    #return average MSE for validation set and testing set\n",
        "    return sum(mse1) / len(mse1), sum(mse2) / len(mse2)\n",
        "\n",
        "\n",
        "def L_fold_with_plot_tree(x,y,N):\n",
        "    mse_for_test = []\n",
        "    mse_for_validation = []\n",
        "    tree_depth = []\n",
        "    #for tree depth from 1 to 15\n",
        "    for i in range(1,15):\n",
        "        tree_depth.append(i)\n",
        "        mse1, mse2 = get_average_accuracy_fold_tree(N,i, x, y)\n",
        "        mse_for_validation.append(mse1)\n",
        "        mse_for_test.append(mse2)\n",
        "    #get the standard deviation\n",
        "    std = statistics.stdev(mse_for_validation)\n",
        "    plt.plot(tree_depth, mse_for_test, label=\"test\")\n",
        "    plt.errorbar(tree_depth, mse_for_validation, std, label=\"validation\")\n",
        "    plt.title('Plot of the mean and standard deviation in 10 fold cross-validation')\n",
        "    plt.xlabel('Tree depth')\n",
        "    plt.ylabel('mean squared error')\n",
        "    plt.xticks(range(0,15,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Boundary - model with KNN"
      ],
      "metadata": {
        "id": "vAsvjEQKeQLw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9VePj6Wpqhb"
      },
      "outputs": [],
      "source": [
        "#plot the decison boundary with knn model\n",
        "def decision_boundary_KNN(dataset1):\n",
        "    max_accuracy = 0\n",
        "    two_features = [None, None]\n",
        "    cont_feature=['AGE','BILIRUBIN','ALK PHOSPHATE','SGOT','ALBUMIN','PROTIME']\n",
        "    j = 1\n",
        "    max_j = 0\n",
        "    max_k = 0\n",
        "    while j<19:\n",
        "        k = j + 1\n",
        "        while k<=19:\n",
        "            y = dataset1.iloc[:, 0].to_numpy()\n",
        "            x1 = dataset1.iloc[:, j].to_numpy()\n",
        "            x2 = dataset1.iloc[:, k].to_numpy()\n",
        "            x = np.concatenate((x1[:,None], x2[:,None]), axis = 1)\n",
        "            (N,D), C = x.shape, np.max(y)+1                                                    \n",
        "\n",
        "            model = KNN(K=6)\n",
        "            accuracy_list=[]\n",
        "            inds = np.random.permutation(N)\n",
        "            accuracies = 0\n",
        "            for i in range(100):\n",
        "        \n",
        "                inds = np.random.permutation(N)  \n",
        "                #split the dataset into train and test(80% training & 20% testing)\n",
        "                x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "                x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "                y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "\n",
        "                #get predictions by choosing the class with the maximum probability\n",
        "                y_pred = np.argmax(y_prob,axis=-1)        \n",
        "                accuracy_list.append(np.sum(y_pred == y_test)/y_test.shape[0])\n",
        "\n",
        "                accuracy = sum(accuracy_list) /len(accuracy_list)\n",
        "                accuracies += accuracy\n",
        "            accuracies /= 100\n",
        "            if(accuracies>max_accuracy and (dataset1.columns[j] in cont_feature) and (dataset1.columns[k] in cont_feature)):\n",
        "                max_accuracy = accuracies\n",
        "                two_features[0] = dataset1.columns[j]\n",
        "                two_features[1] = dataset1.columns[k]\n",
        "                max_j = j\n",
        "                max_k = k\n",
        "            k = k + 1\n",
        "        j = j + 1\n",
        "    print(f'accuracy is {max_accuracy*100:.1f}.')\n",
        "    print(\"The features that used are \" + two_features[0] + \" and \" + two_features[1])\n",
        "\n",
        "    y = dataset1.iloc[:, 0].to_numpy()\n",
        "    x_1 = dataset1.iloc[:, max_j].to_numpy()\n",
        "    x_2 = dataset1.iloc[:, max_k].to_numpy()\n",
        "    x = np.concatenate((x_1[:,None], x_2[:,None]), axis = 1)\n",
        "\n",
        "    (N,D), C = x.shape, np.max(y)+1\n",
        "    inds = np.random.permutation(N)\n",
        "    x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "    x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "\n",
        "    x0v = np.linspace(np.min(x[:, 0]), np.max(x[:, 0]), 200)\n",
        "    x1v = np.linspace(np.min(x[:, 1]), np.max(x[:, 1]), 200)\n",
        "    x0, x1 = np.meshgrid(x0v, x1v)\n",
        "    x_all = np.vstack((x0.ravel(), x1.ravel())).T\n",
        "\n",
        "    model = KNN(K=6)\n",
        "    y_train_prob = np.zeros((y_train.shape[0], C+1))\n",
        "    y_train_prob[np.arange(y_train.shape[0]), y_train] = 1\n",
        "    y_prob_all, _ = model.fit(x_train, y_train).predict(x_all)\n",
        "    y_pred_all = np.zeros((y_prob_all.shape[0], C+1))\n",
        "    y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1\n",
        "\n",
        "    plt.scatter(x_train[:,0], x_train[:,1], c=y_train_prob, marker='o', alpha=1)\n",
        "    plt.scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=0.01)\n",
        "    plt.ylabel(dataset1.columns[max_k])\n",
        "    plt.xlabel(dataset1.columns[max_j])\n",
        "    plt.show()\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree with different cost function"
      ],
      "metadata": {
        "id": "S8_pIgIVsX1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction_cost(N, depth, x, y):\n",
        "    # bulid decision tree model\n",
        "    tree1 = DecisionTree(max_depth=depth)\n",
        "    tree2 = DecisionTree(max_depth=depth, cost_fn = cost_entropy) \n",
        "    tree3 = DecisionTree(max_depth=depth, cost_fn = cost_gini_index)\n",
        "    accuracy_list1=[]\n",
        "    accuracy_list2=[]\n",
        "    accuracy_list3 = []\n",
        "    for i in range(30):\n",
        "        inds = np.random.permutation(N)\n",
        "        # split the dataset into train and test(80% training & 20% testing)\n",
        "        x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "        x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "        #build dicision model with different cost function\n",
        "        probs_test1 = tree1.fit(x_train, y_train).predict(x_test)\n",
        "        probs_test2 = tree2.fit(x_train, y_train).predict(x_test)\n",
        "        probs_test3 = tree3.fit(x_train, y_train).predict(x_test)\n",
        "        y_pred1 = np.argmax(probs_test1,1)\n",
        "        y_pred2 = np.argmax(probs_test2,1)\n",
        "        y_pred3 = np.argmax(probs_test3,1)\n",
        "        accuracy_list1.append(np.sum(y_pred1 == y_test) / y_test.shape[0])\n",
        "        accuracy_list2.append(np.sum(y_pred2 == y_test) / y_test.shape[0])\n",
        "        accuracy_list3.append(np.sum(y_pred3 == y_test) / y_test.shape[0])\n",
        "    #compute the average accuracy for each model\n",
        "    accuracy1 = sum(accuracy_list1) / len(accuracy_list1)\n",
        "    accuracy2 = sum(accuracy_list2) / len(accuracy_list2)\n",
        "    accuracy3 = sum(accuracy_list3) / len(accuracy_list3)\n",
        "    return accuracy1,accuracy2,accuracy3\n",
        "\n",
        "#generate a plot that shows the accuracy of model with different cost function \n",
        "def compare_diff_cost_func(x,y,N):\n",
        "    depth_list=[]\n",
        "    accuracy_misclassification_list=[]\n",
        "    accuracy_entropy_list=[]\n",
        "    accuracy_gini_index_list=[]\n",
        "    for k in range(1,21):\n",
        "        accuracy_misclassification, accuracy_entropy, accuracy_gini_index=make_prediction_cost(N, k, x, y)\n",
        "        accuracy_misclassification_list.append(accuracy_misclassification)\n",
        "        accuracy_entropy_list.append(accuracy_entropy)\n",
        "        accuracy_gini_index_list.append(accuracy_gini_index)\n",
        "        depth_list.append(k)\n",
        "    plt.plot(depth_list, accuracy_misclassification_list, label=\"misclassification\")\n",
        "    plt.plot(depth_list, accuracy_entropy_list, label=\"entropy\")\n",
        "    plt.plot(depth_list, accuracy_gini_index_list, label=\"gini_index\")\n",
        "    plt.title('Plot of accuracy for model#1 with different cost function')\n",
        "    plt.xlabel('Tree Depth')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(range(0,21,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n"
      ],
      "metadata": {
        "id": "sbdCvp0erOXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision boundary - model with Decision Tree"
      ],
      "metadata": {
        "id": "ei7HuTqBvfGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_boundary_tree(dataset1):\n",
        "    max_accuracy = 0\n",
        "    two_features = [None, None]\n",
        "    j = 1\n",
        "    max_j = 0\n",
        "    max_k = 0\n",
        "    cont_feature=['AGE','BILIRUBIN','ALK PHOSPHATE','SGOT','ALBUMIN','PROTIME']\n",
        "    while j < 19:\n",
        "        k = j + 1\n",
        "        while k <= 19:\n",
        "            y = dataset1.iloc[:, 0].to_numpy()\n",
        "            x1 = dataset1.iloc[:, j].to_numpy()\n",
        "            x2 = dataset1.iloc[:, k].to_numpy()\n",
        "            x = np.concatenate((x1[:, None], x2[:, None]), axis=1)\n",
        "\n",
        "            (N, D), C = x.shape, np.max(y) + 1\n",
        "            tree1 = DecisionTree(max_depth=4)    \n",
        "            tree2 = DecisionTree(max_depth=4, cost_fn = cost_entropy) \n",
        "            tree3 = DecisionTree(max_depth=4, cost_fn = cost_gini_index)\n",
        "        \n",
        "            accuracy_list = []\n",
        "            inds = np.random.permutation(N)\n",
        "            accuracies = 0\n",
        "            for i in range(100):\n",
        "                inds = np.random.permutation(N)\n",
        "                # split the dataset into train and test(80% training & 20% testing)\n",
        "                x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "                x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "                # build tree model with misclassification cost function\n",
        "                probs_test = tree1.fit(x_train, y_train).predict(x_test)\n",
        "                y_pred = np.argmax(probs_test,1)\n",
        "                accuracy_list.append(np.sum(y_pred == y_test) / y_test.shape[0])\n",
        "                accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "                accuracies += accuracy\n",
        "            accuracies /= 100\n",
        "            \n",
        "            if (accuracies > max_accuracy and (dataset1.columns[j] in cont_feature) and (dataset1.columns[k] in cont_feature)):\n",
        "                max_accuracy = accuracies\n",
        "                two_features[0] = dataset1.columns[j]\n",
        "                two_features[1] = dataset1.columns[k]\n",
        "                max_j = j\n",
        "                max_k = k\n",
        "            k = k + 1\n",
        "        j = j + 1\n",
        "    #print out the two continuous features were selected with accuracy\n",
        "    print(f'accuracy is {max_accuracy * 100:.1f}.')\n",
        "    print(\"The features that used are \" + two_features[0] + \" and \" + two_features[1])\n",
        "\n",
        "    y = dataset1.iloc[:, 0].to_numpy()\n",
        "    x_1 = dataset1.iloc[:, max_j].to_numpy()\n",
        "    x_2 = dataset1.iloc[:, max_k].to_numpy()\n",
        "    x = np.concatenate((x_1[:, None], x_2[:, None]), axis=1)\n",
        "\n",
        "    (N, D), C = x.shape, np.max(y) + 1\n",
        "    inds = np.random.permutation(N)\n",
        "    x_train, y_train = x[inds[:64]], y[inds[:64]]\n",
        "    x_test, y_test = x[inds[64:]], y[inds[64:]]\n",
        "\n",
        "    x0v = np.linspace(np.min(x[:, 0]), np.max(x[:, 0]), 200)\n",
        "    x1v = np.linspace(np.min(x[:, 1]), np.max(x[:, 1]), 200)\n",
        "    x0, x1 = np.meshgrid(x0v, x1v)\n",
        "    x_all = np.vstack((x0.ravel(), x1.ravel())).T\n",
        "\n",
        "    tree1 = DecisionTree(max_depth=4) \n",
        "    y_train_prob = np.zeros((y_train.shape[0], C + 1))\n",
        "    y_train_prob[np.arange(y_train.shape[0]), y_train] = 1\n",
        "    y_prob_all= tree1.fit(x_train, y_train).predict(x_all)\n",
        "    y_pred_all = np.zeros((y_prob_all.shape[0], C + 1))\n",
        "    y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1\n",
        "\n",
        "    #plot the decision boundary graph\n",
        "    plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train_prob, marker='o', alpha=1)\n",
        "    plt.scatter(x_all[:, 0], x_all[:, 1], c=y_pred_all, marker='.', alpha=0.01)\n",
        "    plt.ylabel(dataset1.columns[max_k])\n",
        "    plt.xlabel(dataset1.columns[max_j])\n",
        "    plt.show()\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "Itk9cgFkvd_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the main function"
      ],
      "metadata": {
        "id": "L2fLUN12Ozk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove comment label to run any function\n",
        "def main():\n",
        "    #loading and pre-clean the raw data\n",
        "    dataset1=load_and_clean_data()\n",
        "\n",
        "    #scaled data into range [0,1] and split the set to feature set x and label set y\n",
        "    x,y,N=scaled_data(dataset1)\n",
        "\n",
        "    #data without scale and split the set to feature set x and label set y\n",
        "    #x,y,N=unscaled_data(dataset1)\n",
        "\n",
        "    #print the accuracy of knn model with specific num of neighbor\n",
        "    #model_knn_for_specific_k(x,y,N,8)\n",
        "\n",
        "    #generate a plot that shows the accuracy of model with different distance function \n",
        "    #compare_diff_distance_func(x,y,N)\n",
        "\n",
        "    #implement the 8-fold cross validation of knn model on scaled data\n",
        "    #L_fold_with_plot_knn(x,y,N)\n",
        "\n",
        "    #print the accuracy of decision tree model with specific tree depth\n",
        "    #model_tree_for_specific_depth(x,y,N,4)\n",
        "\n",
        "    #implement the 8-fold cross validation of decision tree model on scaled data\n",
        "    #L_fold_with_plot_tree(x,y,N)\n",
        "\n",
        "    #generate a plot that shows the accuracy of model with different cost function \n",
        "    #compare_diff_cost_func(x,y,N)\n",
        "\n",
        "    #polt the decison boundary with knn model between two continuous features, where using these two feature will result in a high accuracy\n",
        "    #decision_boundary_KNN(dataset1)\n",
        "\n",
        "    #polt the decison boundary with decision tree model between two continuous features, where using these two feature will result in a high accuracy\n",
        "    #decision_boundary_tree(dataset1)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zephUZpAO0If",
        "outputId": "fbbcc357-832b-4487-b15d-dcad2225fbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Class  AGE  SEX  STEROID  ...  SGOT  ALBUMIN  PROTIME  HISTOLOGY\n",
            "0       1   34    1        2  ...    28      4.0       75          1\n",
            "1       1   39    1        1  ...    30      4.4       85          1\n",
            "2       1   32    1        2  ...   249      3.7       54          1\n",
            "3       1   41    1        2  ...    60      3.9       52          1\n",
            "4       1   30    1        2  ...   144      4.9       78          1\n",
            "..    ...  ...  ...      ...  ...   ...      ...      ...        ...\n",
            "75      1   45    1        2  ...    44      4.2       85          2\n",
            "76      0   49    1        1  ...    70      3.5       35          2\n",
            "77      1   31    1        1  ...   173      4.2       54          2\n",
            "78      1   53    2        1  ...    19      4.1       48          2\n",
            "79      0   43    1        2  ...    19      3.1       42          2\n",
            "\n",
            "[80 rows x 20 columns]\n",
            "    Class       AGE  SEX  STEROID  ...      SGOT  ALBUMIN  PROTIME  HISTOLOGY\n",
            "0       1  0.472222    0        1  ...  0.066667     0.80     0.75          0\n",
            "1       1  0.541667    0        0  ...  0.071429     0.88     0.85          0\n",
            "2       1  0.444444    0        1  ...  0.592857     0.74     0.54          0\n",
            "3       1  0.569444    0        1  ...  0.142857     0.78     0.52          0\n",
            "4       1  0.416667    0        1  ...  0.342857     0.98     0.78          0\n",
            "..    ...       ...  ...      ...  ...       ...      ...      ...        ...\n",
            "75      1  0.625000    0        1  ...  0.104762     0.84     0.85          1\n",
            "76      0  0.680556    0        0  ...  0.166667     0.70     0.35          1\n",
            "77      1  0.430556    0        0  ...  0.411905     0.84     0.54          1\n",
            "78      1  0.736111    1        0  ...  0.045238     0.82     0.48          1\n",
            "79      0  0.597222    0        1  ...  0.045238     0.62     0.42          1\n",
            "\n",
            "[80 rows x 20 columns]\n",
            "instances (N) \t 80 \n",
            "features (D) \t 19 \n",
            "classes (C) \t 2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MiniProject1_Hepatitis",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}