{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comp551A2Data2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import data - Sentiment140 dataset"
      ],
      "metadata": {
        "id": "HDtMX_tcegPI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4wWlDf5ecl2",
        "outputId": "36232198-7685-4667-dc5b-0efa590b18d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-06 20:27:41--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]\n",
            "--2022-03-06 20:27:42--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘data/trainingandtestdata.zip’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  32.7MB/s    in 2.4s    \n",
            "\n",
            "2022-03-06 20:27:44 (32.7 MB/s) - ‘data/trainingandtestdata.zip’ saved [81363704/81363704]\n",
            "\n",
            "Archive:  data/trainingandtestdata.zip\n",
            "  inflating: data/testdata.manual.2009.06.14.csv  \n",
            "  inflating: data/training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ],
      "source": [
        "# loading the train & test csv file from link in pdf\n",
        "!mkdir -p data\n",
        "!wget -nc http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip -P data\n",
        "!unzip -n -d data data/trainingandtestdata.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocess"
      ],
      "metadata": {
        "id": "OeCyKZxUewTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import scipy\n",
        "from scipy.special import factorial\n",
        "from scipy.sparse import vstack\n",
        "import math\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from warnings import filterwarnings\n",
        "\n",
        "def data_process():\n",
        "    #dataframe of training set\n",
        "    df_train = pd.read_csv(\"data/training.1600000.processed.noemoticon.csv\", names=['polarity', 'id', 'date', 'query', 'user', 'text'], encoding='latin-1')\n",
        "    df_train = df_train[['polarity', 'text']]\n",
        "    #dataframe of testing set\n",
        "    df_test = pd.read_csv(\"data/testdata.manual.2009.06.14.csv\", names=['polarity', 'id', 'date', 'query', 'user', 'text'])\n",
        "    df_test = df_test.loc[df_test.polarity!=2].reset_index()\n",
        "    df_test = df_test[['polarity', 'text']]\n",
        "    #delete links and numbers in text\n",
        "    df_train['text']=df_train['text'].apply(lambda x: re.sub('http://\\S+|https://\\S+|@\\S+|[0-9]\\S+','', str(x)))\n",
        "    df_test['text']=df_test['text'].apply(lambda x: re.sub('http://\\S+|https://\\S+|@\\S+|[0-9]\\S+','', str(x)))\n",
        "    #delete special characters from text\n",
        "    df_train['text']=df_train['text'].apply(lambda x: re.sub('[^A-Za-z]+',' ', str(x)))\n",
        "    df_test['text']=df_test['text'].apply(lambda x: re.sub('[^A-Za-z]+',' ', str(x)))\n",
        "    #remove duplicate rows\n",
        "    df_train.drop_duplicates(\"text\", inplace=True)\n",
        "    df_test.drop_duplicates(\"text\", inplace=True)\n",
        "    #modify the column polarity with binary value\n",
        "    df_train.polarity = df_train.polarity.replace({0: 0, 4: 1})\n",
        "    df_test.polarity = df_test.polarity.replace({0: 0, 4: 1})\n",
        "\n",
        "    return df_train, df_test\n"
      ],
      "metadata": {
        "id": "R-hMnObaem9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLass Distribution"
      ],
      "metadata": {
        "id": "LhIPE56WfJm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a bar graph to show the class distribution of the dataset#1\n",
        "def plot_class_distribution(df_train, df_test):\n",
        "    polarity, number1 = np.unique(df_train.polarity, return_counts=True)\n",
        "    target, number2 = np.unique(df_test.polarity, return_counts=True)\n",
        "\n",
        "    index = ['Negative', 'Positive']\n",
        "    df = pd.DataFrame({'train': [number1[0],number1[1]],'test': [number2[0],number2[1]]}, index=index)\n",
        "    ax = df.plot.bar(rot=0,subplots=True,title=\"The plot of class distribution in Sentiment140 dataset\",layout=(1,2),legend=False)\n"
      ],
      "metadata": {
        "id": "YQFWKt-OBg9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up continuous features (tfidf) for Gaussian Naive Bayes"
      ],
      "metadata": {
        "id": "USQn1tHGodP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#turn the text data of training/test set (exclude stop words) into numerical features (computing word tfidf)\n",
        "df_train, df_test = data_process()\n",
        "count_vect = TfidfVectorizer(stop_words='english',max_df=0.5,min_df=5,max_features=1000)\n",
        "train_feature_count = count_vect.fit_transform(df_train.text)\n",
        "test_feature_count = count_vect.transform(df_test.text)\n",
        "#check dictionary of words with frequency\n",
        "print(count_vect.vocabulary_)"
      ],
      "metadata": {
        "id": "bib1dml6fHCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up \"count\" features for Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "T9gviVR8oz2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#turn the text data of training/test set (exclude stop words) into numerical features (counting word frequency)\n",
        "count_vect1 = CountVectorizer(stop_words='english',max_df=0.5,min_df=5,max_features=1000)\n",
        "train_feature_count1 = count_vect1.fit_transform(df_train.text)\n",
        "test_feature_count1 = count_vect1.transform(df_test.text)"
      ],
      "metadata": {
        "id": "cakWByIaFyQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian and Multinomial Naive Bayes Implementation"
      ],
      "metadata": {
        "id": "BcV7fYgm9BHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian(self, xt):\n",
        "    Nt, D = xt.shape\n",
        "    log_prior = np.log(self.pi)[:, None]\n",
        "    threshold=+1e-20\n",
        "    log_likelihood = -.5 * np.log(2*np.pi) - np.log(self.sigma[:,None,:]+threshold) -.5 * (((xt[None,:,:] - self.mu[:,None,:])/(self.sigma[:,None,:]+threshold))**2)\n",
        "    # now we sum over the feature dimension to get a C x N matrix (this has the log-likelihood for each class-test point combination)\n",
        "    log_likelihood = np.sum(log_likelihood, axis=2)\n",
        "    # posterior calculation\n",
        "    log_posterior = log_prior + log_likelihood\n",
        "    posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
        "    return log_posterior.T\n",
        "\n",
        "def multinomial(self, xt):\n",
        "    log_posteriors = np.zeros((xt.shape[0],2))\n",
        "    for i in range(xt.shape[0]):\n",
        "        log_posteriors[i,:]=(((self.likelihood)*(xt[i,:].T)).sum(axis=1)).T+(self.prior).T\n",
        "    return log_posteriors\n",
        "\n",
        "class NaiveBayes:\n",
        "    \n",
        "    def __init__(self, NB_fn=multinomial):\n",
        "        self.NB_fn = NB_fn\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y, alpha=0.5):\n",
        "        #N: number of instance, D:number of feature\n",
        "        N, D = x.shape\n",
        "        #C: number of class\n",
        "        C = np.max(y) + 1\n",
        "        #one parameter for each feature conditioned on each class\n",
        "        mu, sigma = np.zeros((C,D)), np.zeros((C,D))\n",
        "        prior = np.zeros(C)\n",
        "        likelihood = np.zeros((C, D))\n",
        "        Nc = np.zeros(C)\n",
        "        # for each class get the MLE for the mean and std\n",
        "        for c in range(C):\n",
        "            #slice all the elements from class c\n",
        "            x_c = x[y == c] \n",
        "            #get number of elements of class c\n",
        "            Nc[c] = x_c.shape[0]\n",
        "            #prepare the prior and likelihood basedon the input training set\n",
        "            prior[c]=np.log(Nc[c]/ N)\n",
        "            likelihood[c, :] = np.log((x_c.sum(axis=0) + alpha)/(x_c.sum(axis=1).sum(axis=0)+alpha*D))\n",
        "            #mean of features of class c\n",
        "            mu[c,:]=x_c.mean(axis=0)\n",
        "            #std of features of class c\n",
        "            x_c_temp = x_c.copy()\n",
        "            x_c_temp.data **= 2\n",
        "            variance=x_c_temp.mean(axis=0) - np.square(mu[c,:])\n",
        "            sigma[c,:] = np.sqrt(variance)\n",
        "\n",
        "        self.alpha=alpha    \n",
        "        self.C=C\n",
        "        self.prior = prior\n",
        "        self.likelihood = likelihood\n",
        "        self.mu = mu                                  \n",
        "        self.sigma = sigma     \n",
        "        #Laplace smoothing (using alpha_c=1 for all c)                        \n",
        "        self.pi = (Nc+1)/(N+C)\n",
        "        return self\n",
        "\n",
        "def logsumexp(Z):                                                \n",
        "    Zmax = np.max(Z,axis=0)[None,:]\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "def predict(self, xt):\n",
        "    posterior = self.NB_fn(self, xt)\n",
        "    return posterior                                              \n",
        "\n",
        "NaiveBayes.predict = predict\n",
        "\n",
        "#function to evaluate the accuracy of the model\n",
        "def evaluate_acc(y_test, y_predict):\n",
        "    accuracy = np.sum(y_predict == y_test)/y_predict.shape[0]\n",
        "    return accuracy\n",
        "\n",
        "#function takes the training data as input and splits it into k folds \n",
        "#where 1 fold for validation set and k-1 folds for training set \n",
        "def cross_validation_split(x_train_origin, y_train_origin, k_folds=5):\n",
        "    index = np.arange(np.shape(x_train_origin)[0])\n",
        "    np.random.shuffle(index)\n",
        "    x_train_origin, y_train_origin = x_train_origin[index], y_train_origin[index]\n",
        "    val_x=[]\n",
        "    val_y=[]\n",
        "    train_x=[]\n",
        "    train_y=[]\n",
        "    for i in range(k_folds):\n",
        "        \n",
        "        fold_len=x_train_origin.shape[0]//k_folds\n",
        "        val_x.append(x_train_origin[i*fold_len:fold_len*(i+1)])\n",
        "        train_x1 = x_train_origin[0*fold_len:i*fold_len]\n",
        "        train_x2 = x_train_origin[fold_len*(i+1):]\n",
        "        train_x.append(vstack([train_x1, train_x2]))\n",
        "        split_y = np.array_split(y_train_origin, k_folds)\n",
        "        val_y.append(split_y.pop(i))\n",
        "        train_y.append(np.concatenate(split_y))  \n",
        "    return val_x, val_y, train_x, train_y\n",
        "\n",
        "#function takes train/validation sets generated above and a given model as input\n",
        "#iterate through each train/validation set and returns the average result of each fold\n",
        "def kfoldCV(x_valid, y_valid, x_train, y_train, model, alpha=0.05):\n",
        "    accuracy_list=[]\n",
        "    for s in range(len(x_valid)):\n",
        "        model.fit(x_train[s], y_train[s], alpha)\n",
        "        y_prob = model.predict(scipy.sparse.csr_matrix.toarray(x_valid[s]))\n",
        "        y_pred = np.argmax(y_prob, 1)\n",
        "        accuracy_list.append(evaluate_acc(y_valid[s], y_pred))\n",
        "    acc = sum(accuracy_list)/len(accuracy_list)\n",
        "    print(accuracy_list)\n",
        "    print(f'test accuracy: {acc}')\n",
        "    return acc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d4r4NGrfkDvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set up the simple logistic model based on whole training set and test the model by test set to get the initial accuracy \n",
        "filterwarnings('ignore') # ignor the convergence warning\n",
        "#logistice model for count features\n",
        "clf = LogisticRegression(random_state=0).fit(train_feature_count, df_train.polarity.to_numpy())\n",
        "y_prob = clf.predict(scipy.sparse.csr_matrix.toarray(test_feature_count))\n",
        "acc = evaluate_acc(df_test.polarity, y_prob)\n",
        "print(f'test accuracy: {acc}')\n",
        "#logistice model for continous features\n",
        "clf=LogisticRegression(random_state=0).fit(train_feature_count1, df_train.polarity.to_numpy())\n",
        "y_prob = clf.predict(scipy.sparse.csr_matrix.toarray(test_feature_count1))\n",
        "acc = evaluate_acc(df_test.polarity, y_prob)\n",
        "print(f'test accuracy: {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py75jfZu8Py9",
        "outputId": "6bdcb206-7853-4252-cbaa-248440868180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.7827298050139275\n",
            "test accuracy: 0.7771587743732591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cross-validation for hyperparameter of Naive Bayes"
      ],
      "metadata": {
        "id": "VGgQX5SHy0HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#find max_feature that will result in the best performance\n",
        "#compare the overall performance for multinomial and guassian Naive Bayes\n",
        "def cv_max_feature(df_train):\n",
        "    #randomly select 100000 instances as the training set to avoiding the memory problem\n",
        "    df_train1=df_train.sample(n=100000,ignore_index=True)\n",
        "    model1 = NaiveBayes(NB_fn=multinomial)\n",
        "    max_feature1=[]\n",
        "    accuracy1=[]\n",
        "    for i in range(1000,15000,2000):\n",
        "        print(i)\n",
        "        max_feature1.append(i)\n",
        "        count_vect = CountVectorizer(stop_words='english', min_df=1,max_features=i)\n",
        "        train_feature_count = count_vect.fit_transform(df_train1.text)\n",
        "        x_valid1, y_valid1, x_train1, y_train1 = cross_validation_split(train_feature_count, df_train1.polarity, 5)\n",
        "        accuracy1.append(kfoldCV(x_valid1, y_valid1, x_train1, y_train1, model1))\n",
        "\n",
        "    model = NaiveBayes(NB_fn=gaussian)\n",
        "    max_feature=[]\n",
        "    accuracy=[]\n",
        "    for i in range(100,7000,500):\n",
        "        print(i)\n",
        "        max_feature.append(i)\n",
        "        tfidf_vect = TfidfVectorizer(stop_words='english', max_features=i)\n",
        "        train_feature_tfidf = tfidf_vect.fit_transform(df_train1.text)\n",
        "        x_valid, y_valid, x_train, y_train = cross_validation_split(train_feature_tfidf, df_train1.polarity, 5)\n",
        "        accuracy.append(kfoldCV(x_valid, y_valid, x_train, y_train, model))\n",
        "    #generate the line graph\n",
        "    plt.plot(max_feature1, accuracy1, label=\"Multinomial Naive Bayes\")\n",
        "    plt.errorbar(max_feature, accuracy, label=\"Gaussian Naive Bayes\")\n",
        "    plt.title('Plot of models with different # features in 5 fold cross-validation')\n",
        "    plt.xlabel('number of features')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#find max_df that will result in the best performance of the Naive Bayes model\n",
        "#compare the overall performance for multinomial and guassian Naive Bayes\n",
        "def cv_max_df(df_train):\n",
        "    #randomly select 100000 instances as the training set to avoiding the memory problem\n",
        "    df_train1=df_train.sample(n=100000,ignore_index=True)\n",
        "    model = NaiveBayes(NB_fn=multinomial)\n",
        "    max_df=[]\n",
        "    accuracy_multinomial=[]\n",
        "    # max_df = 0.1 means ignore the words occur in more than 10% of the documents\n",
        "    for i in range(1,10,2):\n",
        "        i = i/10\n",
        "        print(i)\n",
        "        max_df.append(i)\n",
        "        count_vect1 = CountVectorizer(stop_words='english', max_features=6000, max_df=i)\n",
        "        train_feature_count1 = count_vect1.fit_transform(df_train1.text)\n",
        "        test_feature_count1 = count_vect1.transform(df_train1.text)\n",
        "        x_valid, y_valid, x_train, y_train = cross_validation_split(train_feature_count1, df_train1.polarity, 5)\n",
        "        accuracy_multinomial.append(kfoldCV(x_valid, y_valid, x_train, y_train, model))\n",
        "\n",
        "    accuracy_gaussian=[]\n",
        "    model1 = NaiveBayes(NB_fn=gaussian)\n",
        "    for i in range(1,10,2):\n",
        "        i = i/10\n",
        "        print(i)\n",
        "        tfidf_vect = TfidfVectorizer(stop_words='english', max_features=6000, max_df=i)\n",
        "        train_feature_tfidf = tfidf_vect.fit_transform(df_train1.text)\n",
        "        test_feature_tfidf = tfidf_vect.transform(df_train1.text)\n",
        "        x_valid, y_valid, x_train, y_train = cross_validation_split(train_feature_tfidf,  df_train1.polarity, 5)\n",
        "        accuracy_gaussian.append(kfoldCV(x_valid, y_valid, x_train, y_train, model1)) # 1 indicate using Gaussian NB\n",
        "\n",
        "    #generate the line graph\n",
        "    plt.plot(max_df, accuracy_multinomial, label=\"Multinomial Naive Bayes\")\n",
        "    plt.plot(max_df, accuracy_gaussian, label=\"Gaussian Naive Bayes\")\n",
        "    plt.title('Plot of models with different max_df in 5 fold cross-validation')\n",
        "    plt.xlabel('max_df')\n",
        "    plt.ylabel('accuracy')\n",
        "    #plt.xticks(range(0,30,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#find min_df that will result in the best performance of the Naive Bayes model\n",
        "def cv_min_df(df_train):\n",
        "    model = NaiveBayes(NB_fn=multinomial)\n",
        "    min_df=[]\n",
        "    accuracy=[]\n",
        "    #randomly select 100000 instances as the training set to avoiding the memory problem\n",
        "    df_train1=df_train.sample(n=100000,ignore_index=True)\n",
        "    for i in range(0,20,2):\n",
        "        print(i)\n",
        "        min_df.append(i)\n",
        "        count_vect1 = CountVectorizer(stop_words='english',min_df=i,max_features=6000)\n",
        "        train_feature_count1 = count_vect1.fit_transform(df_train1.text)\n",
        "        x_valid, y_valid, x_train, y_train = cross_validation_split(train_feature_count1, df_train1.polarity, 5)\n",
        "        accuracy.append(kfoldCV(x_valid, y_valid, x_train, y_train, model))\n",
        "\n",
        "    accuracy_gaussian=[]\n",
        "    model1 = NaiveBayes(NB_fn=gaussian)\n",
        "    for i in range(0,20,2):\n",
        "        print(i)\n",
        "        tfidf_vect = TfidfVectorizer(stop_words='english', max_features=6000, min_df=i)\n",
        "        train_feature_tfidf = tfidf_vect.fit_transform(df_train1.text)\n",
        "        test_feature_tfidf = tfidf_vect.transform(df_train1.text)\n",
        "        x_valid, y_valid, x_train, y_train = cross_validation_split(train_feature_tfidf,  df_train1.polarity, 5)\n",
        "        accuracy_gaussian.append(kfoldCV(x_valid, y_valid, x_train, y_train, model1)) # 1 indicate using Gaussian NB\n",
        "\n",
        "    #generate the line graph\n",
        "    plt.plot(min_df, accuracy, label=\"Multinomial Naive Bayes\")\n",
        "    plt.plot(min_df, accuracy_gaussian, label=\"Gaussian Naive Bayes\")\n",
        "    plt.title('Plot of models with different min_df in 5 fold cross-validation')\n",
        "    plt.xlabel('min_df')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#find the best hyperparameter alpha that will result in the best performance of the multinomial Naive Bayes model\n",
        "def cv_alpha(df_train):\n",
        "    model = NaiveBayes(NB_fn=multinomial)\n",
        "    alpha=[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "    accuracy=[]\n",
        "    #randomly select 100000 instances as the training set to avoiding the memory problem\n",
        "    df_train1=df_train.sample(n=100000,ignore_index=True)\n",
        "    for i in alpha:\n",
        "        print(i)\n",
        "        count_vect1 = CountVectorizer(stop_words='english',max_features=6000)\n",
        "        train_feature_count1 = count_vect1.fit_transform(df_train1.text)\n",
        "        x_valid, y_valid, x_train, y_train = cross_validation_split(train_feature_count1, df_train1.polarity, 5)\n",
        "        accuracy.append(kfoldCV(x_valid, y_valid, x_train, y_train, model,i))\n",
        "\n",
        "    #generate the line graph\n",
        "    plt.plot(alpha, accuracy, label=\"Multinomial Naive Bayes\")\n",
        "    plt.title('Plot of models with different alpha in 5 fold cross-validation')\n",
        "    plt.xlabel('alpha')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DY5PHWKTyzYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cross-validation for hyperparameter of Softmax Regression"
      ],
      "metadata": {
        "id": "lYmfDaWcqLY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cv_max_iter(df_train):\n",
        "    #randomly select 100000 instances as the training set to avoiding the memory problem\n",
        "    df_train1=df_train.sample(n=100000,ignore_index=True)\n",
        "    count_vect1 = CountVectorizer(stop_words='english',max_features=6000)\n",
        "    train_feature_count = count_vect1.fit_transform(df_train1.text)\n",
        "    filterwarnings('ignore')\n",
        "    iteration=[]\n",
        "    accuracy=[]\n",
        "    for i in range(0,50,2):\n",
        "        print(i)\n",
        "        iteration.append(i)\n",
        "        a=[]\n",
        "        x_valid1, y_valid1, x_train1, y_train1 = cross_validation_split(train_feature_count, df_train1.polarity, 5)\n",
        "        for s in range(len(x_valid1)):\n",
        "            clf = LogisticRegression(max_iter=i).fit(x_train1[s], y_train1[s])\n",
        "            y_prob = clf.predict(scipy.sparse.csr_matrix.toarray(x_valid1[s]))\n",
        "            acc = evaluate_acc(y_valid1[s], y_prob)\n",
        "            a.append(acc)\n",
        "        print(sum(a)/len(a))\n",
        "        accuracy.append(sum(a)/len(a))\n",
        "\n",
        "    plt.plot(iteration, accuracy, label=\"Logistic Regression\")\n",
        "    plt.title('Plot of softmax models with different max iterations in 5 fold cross-validation')\n",
        "    plt.xlabel('number of iterations')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xticks(range(0,50,2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "def cv_C(df_train):\n",
        "    #randomly select 100000 instances as the training set to avoiding the memory problem\n",
        "    df_train1=df_train.sample(n=100000,ignore_index=True)\n",
        "    count_vect1 = CountVectorizer(stop_words='english',max_features=6000)\n",
        "    train_feature_count = count_vect1.fit_transform(df_train1.text)\n",
        "    filterwarnings('ignore')\n",
        "    c=[]\n",
        "    accuracy=[]\n",
        "    for i in range(1,100,5):\n",
        "        print(i/100)\n",
        "        c.append(i/100)\n",
        "        a=[]\n",
        "        x_valid1, y_valid1, x_train1, y_train1 = cross_validation_split(train_feature_count, df_train1.polarity, 5)\n",
        "        for s in range(len(x_valid1)):\n",
        "            clf = LogisticRegression(max_iter=20,C=i/100).fit(x_train1[s], y_train1[s])\n",
        "            y_prob = clf.predict(scipy.sparse.csr_matrix.toarray(x_valid1[s]))\n",
        "            acc = evaluate_acc(y_valid1[s], y_prob)\n",
        "            a.append(acc)\n",
        "        print(sum(a)/len(a))\n",
        "        accuracy.append(sum(a)/len(a))\n",
        "\n",
        "    plt.plot(c, accuracy, label=\"Logistic Regression\")\n",
        "    plt.title('Plot of softmax models with different C in 5 fold cross-validation')\n",
        "    plt.xlabel('C - Inverse of regularization strength')\n",
        "    plt.ylabel('accuracy')\n",
        "    #plt.xticks(range(0,1,0.1))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def cv_penalty(df_train):\n",
        "    #randomly select 100000 instances as the training set to avoiding the memory problem\n",
        "    df_train1=df_train.sample(n=100000,ignore_index=True)\n",
        "    count_vect1 = CountVectorizer(stop_words='english',max_features=6000)\n",
        "    train_feature_count = count_vect1.fit_transform(df_train1.text)\n",
        "    filterwarnings('ignore')\n",
        "    penalty=['none','l2']\n",
        "    accuracy=[]\n",
        "    for i in penalty:\n",
        "        print(f'the norm of the penalty: {i}')\n",
        "        a=[]\n",
        "        x_valid1, y_valid1, x_train1, y_train1 = cross_validation_split(train_feature_count, df_train1.polarity, 5)\n",
        "        for s in range(len(x_valid1)):\n",
        "            clf = LogisticRegression(max_iter=20,C=0.21,penalty=i).fit(x_train1[s], y_train1[s])\n",
        "            y_prob = clf.predict(scipy.sparse.csr_matrix.toarray(x_valid1[s]))\n",
        "            acc = evaluate_acc(y_valid1[s], y_prob)\n",
        "            a.append(acc)\n",
        "        accuracy=sum(a)/len(a)\n",
        "        print(f'test accuracy: {accuracy}')\n",
        "    "
      ],
      "metadata": {
        "id": "jfw_5mD2qRf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare performance of naive Bayes and softmax regression"
      ],
      "metadata": {
        "id": "Kkxz3WENrDAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filterwarnings('ignore')\n",
        "def cv_NB_SR():\n",
        "    accuracy1=[]\n",
        "    accuracy=[]\n",
        "    num_feature=[]\n",
        "    #randomly select 100000 instances as the training set to avoiding the memory problem\n",
        "    df_train1=df_train.sample(n=100000,ignore_index=True)\n",
        "    model1 = NaiveBayes(NB_fn=multinomial)\n",
        "    for i in range(1000,29000,1000):\n",
        "        print(i)\n",
        "        num_feature.append(i)\n",
        "        count_vect = CountVectorizer(stop_words='english',max_features=i)\n",
        "        train_feature_count = count_vect.fit_transform(df_train1.text)\n",
        "        x_valid1, y_valid1, x_train1, y_train1 = cross_validation_split(train_feature_count, df_train1.polarity, 5)\n",
        "        a1=kfoldCV(x_valid1, y_valid1, x_train1, y_train1, model1, alpha=0.06)\n",
        "        accuracy1.append(a1)\n",
        "        print(f'The test accuracy for multinomial Naive Bayes with best hyperparameter is: {a1}')\n",
        "\n",
        "        a=[]\n",
        "        for s in range(len(x_valid1)):\n",
        "            clf = LogisticRegression().fit(x_train1[s], y_train1[s])\n",
        "            y_prob = clf.predict(scipy.sparse.csr_matrix.toarray(x_valid1[s]))\n",
        "            acc = evaluate_acc(y_valid1[s], y_prob)\n",
        "            a.append(acc)\n",
        "        accuracy.append(sum(a)/len(a))\n",
        "        print(f'The test accuracy for Softmax Regression with best hyperparameter is: {sum(a)/len(a)}')\n",
        "    plt.plot(num_feature, accuracy1, label=\"Multinomial Naive Bayes\")\n",
        "    plt.errorbar(num_feature, accuracy, label=\"Softmax Regression\")\n",
        "    plt.title('Plot of models with different number of features in 5 fold cross-validation')\n",
        "    plt.xlabel('number of features')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5HTWBOvqrCJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare accuracy of the two models as a function of the size of dataset"
      ],
      "metadata": {
        "id": "oCG0AgXbtrnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cv_size_of_dataset():\n",
        "    percent=[]\n",
        "    accuracy1=[]\n",
        "    accuracy=[]\n",
        "\n",
        "    for i in range(2,10,2):\n",
        "        print(f'{i*10} % of the avaliable training set')\n",
        "        percent.append(i/10)\n",
        "        #randomly select 100000*percent instances as the training set( *100000 to avoiding the memory problem\n",
        "        df_train1=df_train.sample(n=int(100000*i/10),ignore_index=True)\n",
        "        model1 = NaiveBayes(NB_fn=multinomial)\n",
        "        count_vect = CountVectorizer(stop_words='english',max_features=6000)\n",
        "        train_feature_count = count_vect.fit_transform(df_train1.text)\n",
        "        x_valid1, y_valid1, x_train1, y_train1 = cross_validation_split(train_feature_count, df_train1.polarity, 5)\n",
        "        accuracy1.append(kfoldCV(x_valid1, y_valid1, x_train1, y_train1, model1, alpha=0.06))\n",
        "        #print(f'The test accuracy for multinomial Naive Bayes is: {accuracy1}')\n",
        "\n",
        "        a=[]\n",
        "        x_valid1, y_valid1, x_train1, y_train1 = cross_validation_split(train_feature_count, df_train1.polarity, 5)\n",
        "        for s in range(len(x_valid1)):\n",
        "            clf = LogisticRegression(max_iter=20,C=0.21,penalty='l2').fit(x_train1[s], y_train1[s])\n",
        "            y_prob = clf.predict(scipy.sparse.csr_matrix.toarray(x_valid1[s]))\n",
        "            acc = evaluate_acc(y_valid1[s], y_prob)\n",
        "            a.append(acc)\n",
        "        accuracy.append(sum(a)/len(a))\n",
        "        #print(f'The test accuracy for Softmax Regression with best hyperparameter is: {accuracy}')\n",
        "\n",
        "    plt.plot(percent, accuracy1, label=\"Multinomial Naive Bayes\")\n",
        "    plt.errorbar(percent, accuracy, label=\"Softmax Regression\")\n",
        "    plt.title('Plot of models with different percent of training data in 5 fold cross-validation')\n",
        "    plt.xlabel('percent of training data')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qmkOBHVAtr_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Function"
      ],
      "metadata": {
        "id": "KCrKQIkio7JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    df_train, df_test = data_process()\n",
        "    plot_class_distribution(df_train, df_test)\n",
        "    #model = NaiveBayes()\n",
        "\n",
        "    np.seterr(all = 'ignore') \n",
        "    \n",
        "    #cv_max_feature(df_train)\n",
        "    #cv_max_df(df_train)\n",
        "    #cv_min_df(df_train)\n",
        "    #cv_alpha(df_train)\n",
        "    #cv_max_iter(df_train)\n",
        "    #cv_C(df_train)\n",
        "    #cv_penalty(df_train)\n",
        "    #cv_NB_SR()\n",
        "    #cv_size_of_dataset()\n",
        "    \"\"\"\n",
        "    model = NaiveBayes()\n",
        "    for k in range(10,11):\n",
        "        x_valid, y_valid, x_train, y_train = cross_validation_split(train_feature_count, df_train.polarity, k)\n",
        "        kfoldCV(x_valid, y_valid, x_train, y_train, model)\n",
        "    \n",
        "    model.fit(train_feature_count1, df_train.polarity.to_numpy())\n",
        "    y_prob = model.predict(scipy.sparse.csr_matrix.toarray(test_feature_count1))\n",
        "\n",
        "    y_pred = np.argmax(y_prob, 1)\n",
        "    print(y_pred)\n",
        "    acc=evaluate_acc(df_test.polarity, y_pred)\n",
        "    \n",
        "    print(f'test accuracy: {acc}')\n",
        "    \"\"\"\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "C8WHSO9bo6mq",
        "outputId": "7eb04e70-e12b-4367-f07d-ea9b25f7e259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}